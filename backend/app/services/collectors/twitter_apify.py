"""åŸºäºApifyçš„Twitteræ•°æ®é‡‡é›†æœåŠ¡"""

import re
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from loguru import logger
from apify_client import ApifyClient
from app.core.config import settings


class TwitterApifyCollector:
    """Apify Twitteré‡‡é›†å™¨ - ä½¿ç”¨ç¬¬ä¸‰æ–¹æœåŠ¡æ›¿ä»£å®˜æ–¹API"""
    
    # ç›‘æ§çš„å…³é”®è¯åˆ—è¡¨ (ä¸åŸTwitteré‡‡é›†å™¨ä¿æŒä¸€è‡´)
    KEYWORDS = [
        "presale",
        "fair launch",
        "new token",
        "airdrop",
        "testnet",
        "mainnet launch",
        "IDO",
        "ICO",
        "token sale",
        "Web3",
        "DeFi",
        "NFT mint",
    ]
    
    # é¡¶çº§åŠ å¯†KOLåˆ—è¡¨
    TOP_KOLS = [
        "VitalikButerin",
        "aantonop",
        "cz_binance",
        "elonmusk",
        "brian_armstrong",
        "justinsuntron",
        "APompliano",
        "TheCryptoLark",
        "inversebrah",
        "CryptoWendyO",
    ]
    
    def __init__(self):
        """åˆå§‹åŒ–Apifyå®¢æˆ·ç«¯"""
        if not settings.APIFY_API_KEY:
            logger.warning("âš ï¸  Apify API key not configured")
            self.client = None
            return
        
        try:
            self.client = ApifyClient(settings.APIFY_API_KEY)
            logger.info("âœ… Apify Twitter collector initialized")
        except Exception as e:
            logger.error(f"Failed to initialize Apify client: {e}")
            self.client = None
    
    def search_tweets(
        self, 
        query: str, 
        max_results: int = 100,
        hours: int = 24
    ) -> List[Dict]:
        """æœç´¢æ¨æ–‡
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•°
            hours: æœç´¢è¿‡å»Nå°æ—¶çš„æ¨æ–‡
            
        Returns:
            æ¨æ–‡åˆ—è¡¨
        """
        if not self.client:
            logger.warning("Apify client not available")
            return []
        
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´
            start_time = datetime.utcnow() - timedelta(hours=hours)
            
            # Apify Twitter Scraperè¾“å…¥å‚æ•°
            run_input = {
                "searchTerms": [query],
                "maxTweets": max_results,
                "includeSearchTerms": True,
                "onlyImage": False,
                "onlyVideo": False,
                "onlyQuote": False,
                "onlyTwitterBlue": False,
                "language": "en",
                "startDate": start_time.strftime("%Y-%m-%d"),
            }
            
            logger.info(f"ğŸ” Searching tweets for: {query} (max: {max_results})")
            
            # è¿è¡Œactor
            run = self.client.actor(settings.APIFY_TWITTER_ACTOR_ID).call(
                run_input=run_input,
                timeout_secs=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            # è·å–ç»“æœ
            tweets = []
            for item in self.client.dataset(run["defaultDatasetId"]).iterate_items():
                normalized = self._normalize_tweet(item)
                if normalized:
                    tweets.append(normalized)
            
            logger.info(f"âœ… Found {len(tweets)} tweets for query: {query}")
            return tweets
            
        except Exception as e:
            logger.error(f"âŒ Error searching tweets: {e}")
            return []
    
    def track_user_tweets(
        self, 
        username: str, 
        max_results: int = 10
    ) -> List[Dict]:
        """è¿½è¸ªKOLçš„æœ€æ–°æ¨æ–‡
        
        Args:
            username: Twitterç”¨æˆ·å
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ¨æ–‡åˆ—è¡¨
        """
        if not self.client:
            return []
        
        try:
            run_input = {
                "handles": [username],
                "tweetsDesired": max_results,
                "includeReplies": False,
                "includeRetweets": False,
            }
            
            logger.info(f"ğŸ“Š Tracking tweets from @{username}")
            
            run = self.client.actor(settings.APIFY_TWITTER_ACTOR_ID).call(
                run_input=run_input,
                timeout_secs=180
            )
            
            tweets = []
            for item in self.client.dataset(run["defaultDatasetId"]).iterate_items():
                normalized = self._normalize_tweet(item)
                if normalized:
                    tweets.append(normalized)
            
            logger.info(f"âœ… Collected {len(tweets)} tweets from @{username}")
            return tweets
            
        except Exception as e:
            logger.error(f"âŒ Error tracking KOL {username}: {e}")
            return []
    
    def _normalize_tweet(self, raw_tweet: Dict) -> Optional[Dict]:
        """æ ‡å‡†åŒ–æ¨æ–‡æ ¼å¼ï¼Œä¸åŸTwitter APIä¿æŒä¸€è‡´
        
        Args:
            raw_tweet: Apifyè¿”å›çš„åŸå§‹æ¨æ–‡æ•°æ®
            
        Returns:
            æ ‡å‡†åŒ–çš„æ¨æ–‡å­—å…¸
        """
        try:
            author = raw_tweet.get("author", {})
            
            return {
                "tweet_id": raw_tweet.get("id"),
                "text": raw_tweet.get("text", ""),
                "created_at": self._parse_date(raw_tweet.get("createdAt")),
                "author_id": author.get("id"),
                "author_username": author.get("userName"),
                "author_verified": author.get("isBlue", False),
                "author_followers": author.get("followers", 0),
                "likes": raw_tweet.get("likeCount", 0),
                "retweets": raw_tweet.get("retweetCount", 0),
                "replies": raw_tweet.get("replyCount", 0),
                "entities": self._extract_entities(raw_tweet),
            }
        except Exception as e:
            logger.warning(f"Failed to normalize tweet: {e}")
            return None
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return None
        try:
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except:
            return None
    
    def _extract_entities(self, tweet: Dict) -> Dict:
        """æå–å®ä½“ä¿¡æ¯ï¼ˆURLsã€hashtagsã€mentionsï¼‰"""
        entities = {
            "urls": [],
            "hashtags": [],
            "mentions": []
        }
        
        # æå–URLs
        urls = tweet.get("urls", [])
        if isinstance(urls, list):
            entities["urls"] = [{"url": url} for url in urls if isinstance(url, str)]
        
        # ä»æ–‡æœ¬æå–hashtags
        text = tweet.get("text", "")
        hashtags = re.findall(r'#(\w+)', text)
        entities["hashtags"] = [{"tag": tag} for tag in hashtags]
        
        # ä»æ–‡æœ¬æå–mentions
        mentions = re.findall(r'@(\w+)', text)
        entities["mentions"] = [{"username": mention} for mention in mentions]
        
        return entities
    
    def monitor_keywords(self, hours: int = 1) -> List[Dict]:
        """ç›‘æ§å…³é”®è¯ç›¸å…³æ¨æ–‡
        
        Args:
            hours: ç›‘æ§è¿‡å»Nå°æ—¶
            
        Returns:
            æ‰€æœ‰å…³é”®è¯çš„æ¨æ–‡åˆ—è¡¨
        """
        all_tweets = []
        
        # ç”±äºApifyæœ‰è°ƒç”¨é™åˆ¶ï¼Œæˆ‘ä»¬ä¼˜åŒ–å…³é”®è¯ç­–ç•¥
        # é€‰æ‹©æœ€é‡è¦çš„5ä¸ªå…³é”®è¯è¿›è¡Œç›‘æ§
        priority_keywords = ["presale", "airdrop", "IDO", "fair launch", "Web3"]
        
        for keyword in priority_keywords:
            # æ„å»ºæŸ¥è¯¢(æ’é™¤è½¬å‘)
            query = f"{keyword} -RT"
            
            tweets = self.search_tweets(
                query=query,
                max_results=30,  # æ¯ä¸ªå…³é”®è¯30æ¡
                hours=hours
            )
            
            all_tweets.extend(tweets)
        
        # å»é‡
        unique_tweets = {tweet["tweet_id"]: tweet for tweet in all_tweets}
        logger.info(f"ğŸ“Š Collected {len(unique_tweets)} unique tweets from keyword monitoring")
        
        return list(unique_tweets.values())
    
    def track_all_kols(self) -> List[Dict]:
        """è¿½è¸ªæ‰€æœ‰é¡¶çº§KOLçš„æ¨æ–‡
        
        Returns:
            æ‰€æœ‰KOLçš„æ¨æ–‡åˆ—è¡¨
        """
        all_tweets = []
        
        # é€‰æ‹©å‰5ä½KOLè¿›è¡Œç›‘æ§ï¼ˆèŠ‚çœAPIé…é¢ï¼‰
        priority_kols = self.TOP_KOLS[:5]
        
        for username in priority_kols:
            tweets = self.track_user_tweets(username, max_results=5)
            all_tweets.extend(tweets)
        
        logger.info(f"ğŸ“Š Collected {len(all_tweets)} tweets from {len(priority_kols)} KOLs")
        return all_tweets
    
    def extract_project_info(self, tweet: Dict) -> Optional[Dict]:
        """ä»æ¨æ–‡ä¸­æå–é¡¹ç›®ä¿¡æ¯
        
        Args:
            tweet: æ¨æ–‡æ•°æ®
            
        Returns:
            é¡¹ç›®ä¿¡æ¯å­—å…¸æˆ–None
        """
        text = tweet.get("text", "").lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é¡¹ç›®ç›¸å…³å…³é”®è¯
        project_indicators = [
            "presale", "launch", "airdrop", "mint", 
            "token", "ico", "ido", "whitelist"
        ]
        
        if not any(indicator in text for indicator in project_indicators):
            return None
        
        # æå–é¡¹ç›®åç§° (ç®€åŒ–ç‰ˆ)
        project_name = self._extract_project_name(tweet)
        if not project_name:
            return None
        
        # è®¡ç®—è´¨é‡åˆ†æ•°
        quality_score = self._calculate_quality_score(tweet)
        
        return {
            "name": project_name,
            "description": tweet.get("text", "")[:500],
            "twitter_handle": self._extract_twitter_handle(tweet),
            "discovered_at": tweet.get("created_at") or datetime.utcnow(),
            "source": "twitter_apify",
            "source_url": f"https://twitter.com/{tweet.get('author_username')}/status/{tweet.get('tweet_id')}",
            "engagement_score": quality_score,
            "metadata": {
                "likes": tweet.get("likes", 0),
                "retweets": tweet.get("retweets", 0),
                "author_followers": tweet.get("author_followers", 0),
            }
        }
    
    def _extract_project_name(self, tweet: Dict) -> Optional[str]:
        """æå–é¡¹ç›®åç§°"""
        text = tweet.get("text", "")
        
        # å°è¯•ä»hashtagsæå–
        entities = tweet.get("entities", {})
        hashtags = entities.get("hashtags", [])
        
        if hashtags:
            # è¿‡æ»¤æ‰é€šç”¨æ ‡ç­¾
            exclude_tags = {"crypto", "web3", "defi", "nft", "blockchain"}
            for hashtag in hashtags:
                tag = hashtag.get("tag", "").lower()
                if tag and tag not in exclude_tags:
                    return hashtag.get("tag")
        
        # å°è¯•ä»æ–‡æœ¬æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰
        # æŸ¥æ‰¾ "project_name is launching" æ¨¡å¼
        patterns = [
            r'(\w+)\s+(?:is|will be|announces)',
            r'(?:introducing|announcing)\s+(\w+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                name = match.group(1)
                if len(name) > 2:  # è‡³å°‘3ä¸ªå­—ç¬¦
                    return name.capitalize()
        
        return None
    
    def _extract_twitter_handle(self, tweet: Dict) -> Optional[str]:
        """æå–Twitter handle"""
        entities = tweet.get("entities", {})
        mentions = entities.get("mentions", [])
        
        if mentions:
            return mentions[0].get("username")
        
        return None
    
    def _calculate_quality_score(self, tweet: Dict) -> float:
        """è®¡ç®—æ¨æ–‡è´¨é‡åˆ†æ•°
        
        åŸºäºç‚¹èµæ•°ã€è½¬å‘æ•°ã€ä½œè€…ç²‰ä¸æ•°ç­‰æŒ‡æ ‡
        
        Returns:
            0-100çš„è´¨é‡åˆ†æ•°
        """
        likes = tweet.get("likes", 0)
        retweets = tweet.get("retweets", 0)
        followers = tweet.get("author_followers", 0)
        verified = tweet.get("author_verified", False)
        
        # å‚ä¸åº¦åˆ†æ•° (0-50)
        engagement_score = min(50, (likes + retweets * 2) / 10)
        
        # ä½œè€…å½±å“åŠ›åˆ†æ•° (0-30)
        influence_score = min(30, followers / 1000)
        
        # è®¤è¯åŠ åˆ† (0-20)
        verified_score = 20 if verified else 0
        
        total_score = engagement_score + influence_score + verified_score
        
        return min(100, total_score)
    
    def collect_and_extract(self, hours: int = 1) -> List[Dict]:
        """é‡‡é›†æ¨æ–‡å¹¶æå–é¡¹ç›®ä¿¡æ¯ï¼ˆä¸»å…¥å£ï¼‰
        
        Args:
            hours: ç›‘æ§è¿‡å»Nå°æ—¶
            
        Returns:
            é¡¹ç›®ä¿¡æ¯åˆ—è¡¨
        """
        logger.info(f"ğŸ” Starting Apify Twitter collection (last {hours} hours)...")
        
        if not self.client:
            logger.error("âŒ Apify client not initialized")
            return []
        
        # 1. ç›‘æ§å…³é”®è¯
        keyword_tweets = self.monitor_keywords(hours=hours)
        
        # 2. è¿½è¸ªKOL
        kol_tweets = self.track_all_kols()
        
        # 3. åˆå¹¶å»é‡
        all_tweets = keyword_tweets + kol_tweets
        unique_tweets = {tweet["tweet_id"]: tweet for tweet in all_tweets}
        
        logger.info(f"ğŸ“Š Collected {len(unique_tweets)} unique tweets")
        
        # 4. æå–é¡¹ç›®ä¿¡æ¯
        projects = []
        for tweet in unique_tweets.values():
            project_info = self.extract_project_info(tweet)
            if project_info:
                projects.append(project_info)
        
        logger.info(f"âœ… Extracted {len(projects)} potential projects")
        return projects


# å…¨å±€é‡‡é›†å™¨å®ä¾‹
twitter_apify_collector = TwitterApifyCollector()
