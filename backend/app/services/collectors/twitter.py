"""Twitteræ•°æ®é‡‡é›†æœåŠ¡"""

import re
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from loguru import logger
import tweepy
from app.core.config import settings


class TwitterCollector:
    """Twitteræ•°æ®é‡‡é›†å™¨"""
    
    # ç›‘æ§çš„å…³é”®è¯åˆ—è¡¨
    KEYWORDS = [
        "presale",
        "fair launch",
        "new token",
        "airdrop",
        "testnet",
        "mainnet launch",
        "IDO",
        "ICO",
        "token sale",
        "Web3",
        "DeFi",
        "NFT mint",
    ]
    
    # é¡¶çº§åŠ å¯†KOLåˆ—è¡¨
    TOP_KOLS = [
        "VitalikButerin",
        "aantonop",
        "CZ_Binance",
        "SBF_FTX",
        "brian_armstrong",
        "justinsuntron",
        "APompliano",
        "TheCryptoLark",
        "inversebrah",
        "CryptoWendyO",
    ]
    
    def __init__(self):
        """åˆå§‹åŒ–Twitterå®¢æˆ·ç«¯"""
        if not settings.TWITTER_BEARER_TOKEN:
            logger.warning("Twitter Bearer Token not configured")
            self.client = None
            return
            
        try:
            self.client = tweepy.Client(
                bearer_token=settings.TWITTER_BEARER_TOKEN,
                wait_on_rate_limit=True
            )
            logger.info("âœ… Twitter client initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Twitter client: {e}")
            self.client = None
    
    def search_recent_tweets(
        self,
        query: str,
        max_results: int = 100,
        hours: int = 24
    ) -> List[Dict]:
        """æœç´¢æœ€è¿‘çš„æ¨æ–‡
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§ç»“æœæ•° (10-100)
            hours: æœç´¢è¿‡å»Nå°æ—¶çš„æ¨æ–‡
            
        Returns:
            æ¨æ–‡åˆ—è¡¨
        """
        if not self.client:
            logger.warning("Twitter client not available")
            return []
        
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´
            start_time = datetime.utcnow() - timedelta(hours=hours)
            
            # æœç´¢æ¨æ–‡
            tweets = self.client.search_recent_tweets(
                query=query,
                max_results=max_results,
                start_time=start_time,
                tweet_fields=["created_at", "public_metrics", "author_id", "entities"],
                expansions=["author_id"],
                user_fields=["username", "verified", "public_metrics"]
            )
            
            if not tweets.data:
                logger.info(f"No tweets found for query: {query}")
                return []
            
            # è§£ææ¨æ–‡æ•°æ®
            results = []
            users = {user.id: user for user in tweets.includes.get("users", [])}
            
            for tweet in tweets.data:
                author = users.get(tweet.author_id)
                
                result = {
                    "tweet_id": tweet.id,
                    "text": tweet.text,
                    "created_at": tweet.created_at,
                    "author_id": tweet.author_id,
                    "author_username": author.username if author else None,
                    "author_verified": author.verified if author else False,
                    "author_followers": author.public_metrics["followers_count"] if author else 0,
                    "likes": tweet.public_metrics["like_count"],
                    "retweets": tweet.public_metrics["retweet_count"],
                    "replies": tweet.public_metrics["reply_count"],
                    "entities": tweet.entities if hasattr(tweet, "entities") else {},
                }
                
                results.append(result)
            
            logger.info(f"âœ… Found {len(results)} tweets for query: {query}")
            return results
            
        except Exception as e:
            logger.error(f"Error searching tweets: {e}")
            return []
    
    def monitor_keywords(self, hours: int = 1) -> List[Dict]:
        """ç›‘æ§å…³é”®è¯ç›¸å…³æ¨æ–‡
        
        Args:
            hours: ç›‘æ§è¿‡å»Nå°æ—¶
            
        Returns:
            æ‰€æœ‰å…³é”®è¯çš„æ¨æ–‡åˆ—è¡¨
        """
        all_tweets = []
        
        for keyword in self.KEYWORDS:
            # æ„å»ºæŸ¥è¯¢(æ’é™¤è½¬å‘)
            query = f"{keyword} -is:retweet lang:en"
            
            tweets = self.search_recent_tweets(
                query=query,
                max_results=50,
                hours=hours
            )
            
            # æ ‡è®°å…³é”®è¯
            for tweet in tweets:
                tweet["matched_keyword"] = keyword
            
            all_tweets.extend(tweets)
        
        logger.info(f"âœ… Total tweets collected: {len(all_tweets)}")
        return all_tweets
    
    def track_kol_tweets(self, username: str, max_results: int = 10) -> List[Dict]:
        """è¿½è¸ªKOLçš„æœ€æ–°æ¨æ–‡
        
        Args:
            username: Twitterç”¨æˆ·å
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ¨æ–‡åˆ—è¡¨
        """
        if not self.client:
            return []
        
        try:
            # è·å–ç”¨æˆ·
            user = self.client.get_user(username=username)
            if not user.data:
                logger.warning(f"User not found: {username}")
                return []
            
            user_id = user.data.id
            
            # è·å–ç”¨æˆ·æ¨æ–‡
            tweets = self.client.get_users_tweets(
                id=user_id,
                max_results=max_results,
                tweet_fields=["created_at", "public_metrics", "entities"],
                exclude=["retweets", "replies"]  # åªè¦åŸåˆ›æ¨æ–‡
            )
            
            if not tweets.data:
                return []
            
            results = []
            for tweet in tweets.data:
                result = {
                    "tweet_id": tweet.id,
                    "text": tweet.text,
                    "created_at": tweet.created_at,
                    "author_username": username,
                    "likes": tweet.public_metrics["like_count"],
                    "retweets": tweet.public_metrics["retweet_count"],
                    "replies": tweet.public_metrics["reply_count"],
                    "entities": tweet.entities if hasattr(tweet, "entities") else {},
                }
                results.append(result)
            
            logger.info(f"âœ… Collected {len(results)} tweets from @{username}")
            return results
            
        except Exception as e:
            logger.error(f"Error tracking KOL {username}: {e}")
            return []
    
    def track_all_kols(self) -> List[Dict]:
        """è¿½è¸ªæ‰€æœ‰é¡¶çº§KOLçš„æ¨æ–‡
        
        Returns:
            æ‰€æœ‰KOLçš„æ¨æ–‡åˆ—è¡¨
        """
        all_tweets = []
        
        for username in self.TOP_KOLS:
            tweets = self.track_kol_tweets(username, max_results=10)
            
            # æ ‡è®°ä¸ºKOLæ¨æ–‡
            for tweet in tweets:
                tweet["is_kol_tweet"] = True
                tweet["kol_username"] = username
            
            all_tweets.extend(tweets)
        
        logger.info(f"âœ… Total KOL tweets collected: {len(all_tweets)}")
        return all_tweets
    
    def extract_project_info(self, tweet: Dict) -> Optional[Dict]:
        """ä»æ¨æ–‡ä¸­æå–é¡¹ç›®ä¿¡æ¯
        
        Args:
            tweet: æ¨æ–‡æ•°æ®
            
        Returns:
            é¡¹ç›®ä¿¡æ¯å­—å…¸ or None
        """
        text = tweet["text"]
        entities = tweet.get("entities", {})
        
        # æå–URL
        urls = []
        if "urls" in entities:
            urls = [url["expanded_url"] for url in entities["urls"]]
        
        # æå–æåŠçš„ç”¨æˆ·
        mentions = []
        if "mentions" in entities:
            mentions = [mention["username"] for mention in entities["mentions"]]
        
        # æå–hashtags
        hashtags = []
        if "hashtags" in entities:
            hashtags = [tag["tag"] for tag in entities["hashtags"]]
        
        # å°è¯•æå–åˆçº¦åœ°å€(ä»¥å¤ªåŠåœ°å€æ ¼å¼)
        contract_pattern = r"0x[a-fA-F0-9]{40}"
        contracts = re.findall(contract_pattern, text)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜æ˜¾çš„é¡¹ç›®ä¿¡æ¯ï¼Œè¿”å›None
        if not urls and not contracts and not hashtags:
            return None
        
        return {
            "tweet_id": tweet["tweet_id"],
            "discovered_at": tweet["created_at"],
            "source": "twitter",
            "source_url": f"https://twitter.com/{tweet.get('author_username')}/status/{tweet['tweet_id']}",
            "text": text,
            "urls": urls,
            "contracts": contracts,
            "hashtags": hashtags,
            "mentions": mentions,
            "engagement": {
                "likes": tweet["likes"],
                "retweets": tweet["retweets"],
                "replies": tweet["replies"],
            },
            "author": {
                "username": tweet.get("author_username"),
                "verified": tweet.get("author_verified", False),
                "followers": tweet.get("author_followers", 0),
            }
        }
    
    def get_tweet_replies(self, tweet_id: str, max_results: int = 100) -> List[Dict]:
        """è·å–æ¨æ–‡çš„è¯„è®º
        
        Args:
            tweet_id: æ¨æ–‡ID
            max_results: æœ€å¤§è¯„è®ºæ•°
            
        Returns:
            è¯„è®ºåˆ—è¡¨
        """
        if not self.client:
            return []
        
        try:
            # æ„å»ºæŸ¥è¯¢ï¼šconversation_idç­‰äºè¯¥æ¨æ–‡IDçš„æ‰€æœ‰å›å¤
            query = f"conversation_id:{tweet_id}"
            
            tweets = self.client.search_recent_tweets(
                query=query,
                max_results=min(max_results, 100),
                tweet_fields=["created_at", "public_metrics", "author_id", "entities"],
                expansions=["author_id"],
                user_fields=["username", "verified", "public_metrics"]
            )
            
            if not tweets.data:
                return []
            
            # è§£æè¯„è®ºæ•°æ®
            results = []
            users = {user.id: user for user in tweets.includes.get("users", [])}
            
            for tweet in tweets.data:
                author = users.get(tweet.author_id)
                
                result = {
                    "comment_id": tweet.id,
                    "text": tweet.text,
                    "created_at": tweet.created_at,
                    "author_username": author.username if author else None,
                    "author_followers": author.public_metrics["followers_count"] if author else 0,
                    "likes": tweet.public_metrics["like_count"],
                    "replies": tweet.public_metrics["reply_count"],
                    "entities": tweet.entities if hasattr(tweet, "entities") else {},
                }
                
                results.append(result)
            
            logger.info(f"âœ… Collected {len(results)} replies for tweet {tweet_id}")
            return results
            
        except Exception as e:
            logger.error(f"Error getting replies for tweet {tweet_id}: {e}")
            return []
    
    def calculate_comment_quality(self, comment: Dict) -> int:
        """è®¡ç®—è¯„è®ºè´¨é‡åˆ†ï¼ˆ0-100ï¼‰
        
        Args:
            comment: è¯„è®ºæ•°æ®
            
        Returns:
            è´¨é‡åˆ†æ•°
        """
        score = 0
        
        # å› å­1: è¯„è®ºè€…ç²‰ä¸æ•°ï¼ˆ30åˆ†ï¼‰
        followers = comment.get("author_followers", 0)
        score += min(30, followers / 1000)
        
        # å› å­2: è¯„è®ºäº’åŠ¨æ•°ï¼ˆ30åˆ†ï¼‰
        engagement = comment.get("likes", 0) + comment.get("replies", 0)
        score += min(30, engagement / 10)
        
        # å› å­3: è¯„è®ºé•¿åº¦ï¼ˆ20åˆ†ï¼‰- é•¿è¯„è®ºå¾€å¾€æ›´æœ‰ä»·å€¼
        text_length = len(comment.get("text", ""))
        if 50 < text_length < 280:
            score += 20
        elif text_length >= 280:
            score += 15
        
        # å› å­4: åŒ…å«é“¾æ¥æˆ–åˆçº¦åœ°å€ï¼ˆ20åˆ†ï¼‰
        text = comment.get("text", "")
        has_url = "http" in text
        has_contract = bool(re.search(r"0x[a-fA-F0-9]{40}", text))
        if has_url or has_contract:
            score += 20
        
        # æƒ©ç½šé¡¹: åƒåœ¾è¯„è®ºç‰¹å¾
        spam_keywords = ["follow me", "dm me", "check my profile", "100x guaranteed"]
        for keyword in spam_keywords:
            if keyword.lower() in text.lower():
                score -= 30
        
        return max(0, min(100, int(score)))
    
    def extract_project_names(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–é¡¹ç›®åç§°
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            é¡¹ç›®åç§°åˆ—è¡¨
        """
        projects = []
        
        # æ–¹æ³•1: æ­£åˆ™åŒ¹é…ï¼ˆå¤§å†™å¼€å¤´çš„è¯ç»„ï¼‰
        pattern1 = r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b'
        matches1 = re.findall(pattern1, text)
        projects.extend(matches1)
        
        # æ–¹æ³•2: è¯†åˆ«ç‰¹å®šæ¨¡å¼
        pattern2 = r'(?:check out|introducing|new project)\s+([A-Z][a-zA-Z\s]+?)(?:\.|,|$)'
        matches2 = re.findall(pattern2, text, re.IGNORECASE)
        projects.extend(matches2)
        
        # æ–¹æ³•3: è¯†åˆ«ç½‘ç«™é“¾æ¥
        urls = re.findall(r'https?://([a-zA-Z0-9-]+)\.[a-z]+', text)
        for url in urls:
            project_name = url.replace('-', ' ').title()
            projects.append(project_name)
        
        # å»é‡å’Œæ¸…æ´—
        projects = list(set([p.strip() for p in projects if p.strip()]))
        
        # è¿‡æ»¤å™ªéŸ³ï¼ˆé€šç”¨è¯æ±‡ï¼‰
        noise_words = ["The", "This", "That", "Twitter", "Discord", "Telegram"]
        projects = [p for p in projects if p not in noise_words and len(p) > 2]
        
        return projects
    
    def mine_comment_section(self, tweet_id: str) -> List[Dict]:
        """æŒ–æ˜è¯„è®ºåŒº
        
        Args:
            tweet_id: æ¨æ–‡ID
            
        Returns:
            å‘ç°çš„é¡¹ç›®æåŠåˆ—è¡¨
        """
        # 1. è·å–æ‰€æœ‰è¯„è®º
        comments = self.get_tweet_replies(tweet_id, max_results=100)
        
        if not comments:
            return []
        
        # 2. è´¨é‡è¿‡æ»¤
        quality_comments = []
        for comment in comments:
            quality = self.calculate_comment_quality(comment)
            if quality > 60:
                comment["quality_score"] = quality
                quality_comments.append(comment)
        
        logger.info(f"ğŸ“Š {len(quality_comments)}/{len(comments)} comments passed quality filter")
        
        # 3. æå–é¡¹ç›®æåŠ
        project_mentions = []
        
        for comment in quality_comments:
            projects = self.extract_project_names(comment["text"])
            
            for project in projects:
                project_mentions.append({
                    "project_name": project,
                    "mentioned_in_comment": comment["comment_id"],
                    "comment_author": comment["author_username"],
                    "comment_likes": comment["likes"],
                    "context": comment["text"][:200],
                    "quality_score": comment["quality_score"],
                    "discovered_at": comment["created_at"]
                })
        
        logger.info(f"âœ… Extracted {len(project_mentions)} project mentions from comments")
        return project_mentions
    
    def mine_hot_tweets_comments(self, hours: int = 6, min_engagement: int = 100) -> List[Dict]:
        """æŒ–æ˜çƒ­é—¨æ¨æ–‡çš„è¯„è®ºåŒº
        
        Args:
            hours: è¿‡å»Nå°æ—¶
            min_engagement: æœ€å°äº’åŠ¨æ•°
            
        Returns:
            æ‰€æœ‰å‘ç°çš„é¡¹ç›®æåŠ
        """
        logger.info(f"ğŸ” Mining comment sections from hot tweets (last {hours} hours)...")
        
        # 1. æœç´¢é«˜äº’åŠ¨çš„åŠ å¯†ç›¸å…³æ¨æ–‡
        query = "(crypto OR web3 OR DeFi OR NFT) min_faves:" + str(min_engagement)
        
        try:
            hot_tweets = self.search_recent_tweets(
                query=query,
                max_results=50,
                hours=hours
            )
            
            logger.info(f"ğŸ“Š Found {len(hot_tweets)} hot tweets")
            
            # 2. æŒ–æ˜æ¯æ¡æ¨æ–‡çš„è¯„è®ºåŒº
            all_mentions = []
            
            for tweet in hot_tweets[:20]:  # é™åˆ¶å¤„ç†å‰20æ¡ï¼Œé¿å…APIé™æµ
                mentions = self.mine_comment_section(tweet["tweet_id"])
                all_mentions.extend(mentions)
            
            logger.info(f"âœ… Total project mentions from comments: {len(all_mentions)}")
            return all_mentions
            
        except Exception as e:
            logger.error(f"Error mining hot tweets comments: {e}")
            return []
    
    def calculate_topic_heat(self, topic: str, time_window: int = 24) -> Dict:
        """è®¡ç®—è¯é¢˜çƒ­åº¦
        
        Args:
            topic: è¯é¢˜æˆ–é¡¹ç›®åç§°
            time_window: æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            çƒ­åº¦æ•°æ®
        """
        try:
            # æœç´¢è¯¥è¯é¢˜çš„æ¨æ–‡
            tweets = self.search_recent_tweets(
                query=topic,
                max_results=100,
                hours=time_window
            )
            
            mention_count = len(tweets)
            
            # è®¡ç®—KOLå‚ä¸åº¦
            kol_count = sum(1 for t in tweets if t.get("author_followers", 0) > 10000)
            
            # è®¡ç®—å¹³å‡äº’åŠ¨æ•°
            if tweets:
                avg_engagement = sum(
                    t.get("likes", 0) + t.get("retweets", 0) + t.get("replies", 0)
                    for t in tweets
                ) / len(tweets)
            else:
                avg_engagement = 0
            
            # è®¡ç®—çƒ­åº¦åˆ†ï¼ˆ0-100ï¼‰
            heat_score = min(100, (
                min(50, mention_count / 10) +  # æåŠæ•°
                min(30, kol_count * 6) +  # KOLå‚ä¸
                min(20, avg_engagement / 50)  # å¹³å‡äº’åŠ¨
            ))
            
            return {
                "topic": topic,
                "mention_count": mention_count,
                "kol_count": kol_count,
                "avg_engagement": int(avg_engagement),
                "heat_score": int(heat_score),
                "is_trending": heat_score > 70,
                "checked_at": datetime.utcnow()
            }
            
        except Exception as e:
            logger.error(f"Error calculating topic heat for {topic}: {e}")
            return {
                "topic": topic,
                "heat_score": 0,
                "error": str(e)
            }
    
    def collect_and_extract(self, hours: int = 1) -> List[Dict]:
        """é‡‡é›†æ¨æ–‡å¹¶æå–é¡¹ç›®ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        Args:
            hours: ç›‘æ§è¿‡å»Nå°æ—¶
            
        Returns:
            é¡¹ç›®ä¿¡æ¯åˆ—è¡¨
        """
        logger.info(f"ğŸ” Starting enhanced Twitter collection (last {hours} hours)...")
        
        # 1. ç›‘æ§å…³é”®è¯
        keyword_tweets = self.monitor_keywords(hours=hours)
        
        # 2. è¿½è¸ªKOL
        kol_tweets = self.track_all_kols()
        
        # 3. æŒ–æ˜è¯„è®ºåŒºï¼ˆæ–°åŠŸèƒ½ï¼‰
        comment_mentions = self.mine_hot_tweets_comments(hours=hours*6, min_engagement=100)
        
        # 4. åˆå¹¶å»é‡
        all_tweets = keyword_tweets + kol_tweets
        unique_tweets = {tweet["tweet_id"]: tweet for tweet in all_tweets}
        
        logger.info(f"ğŸ“Š Collected {len(unique_tweets)} unique tweets")
        logger.info(f"ğŸ’¬ Found {len(comment_mentions)} project mentions from comments")
        
        # 5. æå–é¡¹ç›®ä¿¡æ¯
        projects = []
        for tweet in unique_tweets.values():
            project_info = self.extract_project_info(tweet)
            if project_info:
                project_info["source_type"] = "tweet"
                projects.append(project_info)
        
        # 6. æ·»åŠ è¯„è®ºåŒºå‘ç°çš„é¡¹ç›®
        for mention in comment_mentions:
            projects.append({
                "project_name": mention["project_name"],
                "discovered_at": mention["discovered_at"],
                "source": "twitter_comment",
                "source_type": "comment",
                "context": mention["context"],
                "quality_score": mention["quality_score"]
            })
        
        logger.info(f"âœ… Extracted {len(projects)} potential projects (tweets + comments)")
        return projects


# å…¨å±€é‡‡é›†å™¨å®ä¾‹
twitter_collector = TwitterCollector()

