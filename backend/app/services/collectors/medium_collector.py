"""Mediumæ•°æ®é‡‡é›†æœåŠ¡"""

import re
import feedparser
import requests
from typing import List, Dict, Optional
from datetime import datetime
from loguru import logger
from bs4 import BeautifulSoup


class MediumCollector:
    """Mediumæ•°æ®é‡‡é›†å™¨"""
    
    # RSSè®¢é˜…æº
    RSS_SOURCES = [
        # é¡¶çº§ä½œè€…
        "https://medium.com/feed/@VitalikButerin",
        "https://medium.com/feed/@coinbase",
        
        # æ ‡ç­¾è®¢é˜…
        "https://medium.com/feed/tag/web3",
        "https://medium.com/feed/tag/defi",
        "https://medium.com/feed/tag/cryptocurrency",
        "https://medium.com/feed/tag/blockchain",
    ]
    
    def __init__(self):
        """åˆå§‹åŒ–é‡‡é›†å™¨"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        logger.info("âœ… Medium Collector initialized")
    
    def collect_from_rss(self, rss_url: str, max_results: int = 20) -> List[Dict]:
        """ä»RSSæºé‡‡é›†æ–‡ç« 
        
        Args:
            rss_url: RSSè®¢é˜…åœ°å€
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        try:
            feed = feedparser.parse(rss_url)
            
            articles = []
            for entry in feed.entries[:max_results]:
                article = {
                    "title": entry.get("title", ""),
                    "url": entry.get("link", ""),
                    "author": entry.get("author", ""),
                    "published": entry.get("published_parsed"),
                    "summary": entry.get("summary", ""),
                    "tags": [tag.term for tag in entry.get("tags", [])],
                    "source": "rss",
                    "rss_url": rss_url
                }
                
                # æ£€æŸ¥æ˜¯å¦Web3ç›¸å…³
                if self.is_web3_related(article):
                    articles.append(article)
            
            logger.info(f"âœ… Collected {len(articles)} articles from RSS")
            return articles
            
        except Exception as e:
            logger.error(f"Error collecting from RSS {rss_url}: {e}")
            return []
    
    def is_web3_related(self, article: Dict) -> bool:
        """åˆ¤æ–­æ–‡ç« æ˜¯å¦Web3ç›¸å…³"""
        text = (article.get("title", "") + " " + article.get("summary", "")).lower()
        
        keywords = [
            "web3", "crypto", "blockchain", "defi", "nft",
            "dao", "ethereum", "solana", "layer2", "rollup",
            "metaverse", "gamefi", "socialfi"
        ]
        
        return any(keyword in text for keyword in keywords)
    
    def scrape_article(self, url: str) -> Optional[Dict]:
        """çˆ¬å–æ–‡ç« å…¨æ–‡
        
        Args:
            url: æ–‡ç« URL
            
        Returns:
            æ–‡ç« æ•°æ®
        """
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ ‡é¢˜
            title_tag = soup.find('h1')
            title = title_tag.text if title_tag else ""
            
            # æå–ä½œè€…
            author_tag = soup.find('a', {'rel': 'author'})
            if not author_tag:
                author_tag = soup.find('a', {'data-action': 'show-user-card'})
            author = author_tag.text if author_tag else ""
            
            # æå–å‘å¸ƒæ—¶é—´
            time_tag = soup.find('time')
            published = time_tag['datetime'] if time_tag and 'datetime' in time_tag.attrs else None
            
            # æå–æ­£æ–‡
            article_body = soup.find('article')
            if article_body:
                paragraphs = article_body.find_all('p')
                full_text = '\n\n'.join([p.text for p in paragraphs])
            else:
                full_text = ""
            
            # æå–ç»Ÿè®¡æ•°æ®
            claps = self.extract_claps(soup)
            
            # æå–æåŠçš„é¡¹ç›®
            mentioned_projects = self.extract_project_names(full_text)
            
            return {
                "url": url,
                "title": title,
                "author": author,
                "published": published,
                "full_text": full_text,
                "word_count": len(full_text.split()),
                "claps": claps,
                "mentioned_projects": mentioned_projects,
                "scraped_at": datetime.utcnow()
            }
            
        except Exception as e:
            logger.error(f"Error scraping article {url}: {e}")
            return None
    
    def extract_claps(self, soup) -> int:
        """æå–æ‹æ‰‹æ•°"""
        try:
            clap_button = soup.find('button', {'data-action': 'show-recommends'})
            if clap_button:
                clap_text = clap_button.text
                # æå–æ•°å­—
                numbers = re.findall(r'\d+', clap_text)
                if numbers:
                    # å¤„ç†K, Mç­‰å•ä½
                    if 'K' in clap_text:
                        return int(float(numbers[0]) * 1000)
                    elif 'M' in clap_text:
                        return int(float(numbers[0]) * 1000000)
                    else:
                        return int(numbers[0])
            return 0
        except:
            return 0
    
    def extract_project_names(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–é¡¹ç›®åç§°"""
        projects = []
        
        # æ­£åˆ™åŒ¹é…å¤§å†™å¼€å¤´çš„è¯ç»„
        pattern = r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b'
        matches = re.findall(pattern, text)
        projects.extend(matches)
        
        # å»é‡å’Œæ¸…æ´—
        projects = list(set([p.strip() for p in projects if len(p) > 2]))
        
        # è¿‡æ»¤å¸¸è§è¯æ±‡
        noise_words = [
            "The", "This", "That", "Medium", "Twitter", "Facebook",
            "Google", "Apple", "Microsoft", "Amazon"
        ]
        projects = [p for p in projects if p not in noise_words]
        
        # é™åˆ¶æ•°é‡
        return projects[:50]
    
    def analyze_article(self, article: Dict) -> Dict:
        """åˆ†ææ–‡ç« ï¼ˆä½¿ç”¨AIï¼‰
        
        Args:
            article: æ–‡ç« æ•°æ®
            
        Returns:
            åˆ†æç»“æœ
        """
        # TODO: é›†æˆAIåˆ†æ
        
        text = article.get("full_text", "")
        
        # ç®€å•çš„å…³é”®è¯åˆ†æ
        article_type = "General"
        if any(kw in text.lower() for kw in ["technical", "architecture", "protocol"]):
            article_type = "Technical"
        elif any(kw in text.lower() for kw in ["investment", "valuation", "token"]):
            article_type = "Investment"
        elif any(kw in text.lower() for kw in ["update", "progress", "milestone"]):
            article_type = "Update"
        
        # è®¡ç®—é‡è¦æ€§
        importance = self.calculate_importance(article)
        
        return {
            "article_url": article["url"],
            "article_type": article_type,
            "mentioned_projects": article.get("mentioned_projects", []),
            "importance_score": importance,
            "has_alpha": importance > 80,
            "analyzed_at": datetime.utcnow()
        }
    
    def calculate_importance(self, article: Dict) -> int:
        """è®¡ç®—æ–‡ç« é‡è¦æ€§ï¼ˆ0-100ï¼‰"""
        score = 0
        
        # å› å­1: æ‹æ‰‹æ•°ï¼ˆ30åˆ†ï¼‰
        claps = article.get("claps", 0)
        score += min(30, claps / 1000 * 30)
        
        # å› å­2: å­—æ•°ï¼ˆ20åˆ†ï¼‰
        word_count = article.get("word_count", 0)
        if 1000 < word_count < 5000:
            score += 20
        elif word_count >= 5000:
            score += 15
        
        # å› å­3: ä½œè€…ï¼ˆ20åˆ†ï¼‰
        author = article.get("author", "").lower()
        if "vitalik" in author or "coinbase" in author:
            score += 20
        
        # å› å­4: é¡¹ç›®æåŠæ•°é‡ï¼ˆ15åˆ†ï¼‰
        projects = article.get("mentioned_projects", [])
        score += min(15, len(projects) * 3)
        
        # å› å­5: å‘å¸ƒæ—¶é—´ï¼ˆ15åˆ†ï¼‰
        # è¶Šæ–°çš„æ–‡ç« è¶Šé‡è¦
        published = article.get("published")
        if published:
            try:
                pub_time = datetime(*published[:6]) if isinstance(published, tuple) else datetime.fromisoformat(published)
                age_hours = (datetime.utcnow() - pub_time).total_seconds() / 3600
                if age_hours < 24:
                    score += 15
                elif age_hours < 72:
                    score += 10
                elif age_hours < 168:
                    score += 5
            except:
                pass
        
        return min(100, int(score))
    
    def collect_all_rss_sources(self, max_articles_per_source: int = 20) -> List[Dict]:
        """é‡‡é›†æ‰€æœ‰RSSæº
        
        Args:
            max_articles_per_source: æ¯ä¸ªæºæœ€å¤§æ–‡ç« æ•°
            
        Returns:
            æ‰€æœ‰æ–‡ç« åˆ—è¡¨
        """
        logger.info(f"ğŸ” Collecting from {len(self.RSS_SOURCES)} RSS sources...")
        
        all_articles = []
        
        for rss_url in self.RSS_SOURCES:
            articles = self.collect_from_rss(rss_url, max_articles_per_source)
            all_articles.extend(articles)
        
        logger.info(f"âœ… Collected {len(all_articles)} articles from all RSS sources")
        return all_articles
    
    def collect_and_analyze(self, scrape_full_text: bool = True) -> List[Dict]:
        """é‡‡é›†å¹¶åˆ†ææ–‡ç« 
        
        Args:
            scrape_full_text: æ˜¯å¦çˆ¬å–å…¨æ–‡
            
        Returns:
            åˆ†æåçš„æ–‡ç« åˆ—è¡¨
        """
        logger.info("ğŸ” Starting Medium collection...")
        
        # 1. ä»RSSé‡‡é›†
        articles = self.collect_all_rss_sources()
        
        # 2. çˆ¬å–å…¨æ–‡ï¼ˆå¯é€‰ï¼‰
        if scrape_full_text:
            logger.info("ğŸ“„ Scraping full text for high-priority articles...")
            
            # åªçˆ¬å–å‰10ç¯‡ï¼ˆé¿å…è¿‡åº¦è¯·æ±‚ï¼‰
            for article in articles[:10]:
                full_article = self.scrape_article(article["url"])
                if full_article:
                    article.update(full_article)
        
        # 3. åˆ†ææ–‡ç« 
        analyzed_articles = []
        for article in articles:
            analysis = self.analyze_article(article)
            
            # åˆå¹¶æ•°æ®
            article["analysis"] = analysis
            analyzed_articles.append(article)
        
        # 4. æŒ‰é‡è¦æ€§æ’åº
        analyzed_articles.sort(
            key=lambda x: x["analysis"].get("importance_score", 0),
            reverse=True
        )
        
        logger.info(f"âœ… Analyzed {len(analyzed_articles)} articles")
        return analyzed_articles


# å…¨å±€é‡‡é›†å™¨å®ä¾‹
medium_collector = MediumCollector()

