"""é¡¹ç›®å‘ç°æœåŠ¡ - ä»å¤šå¹³å°æ•°æ®ä¸­å‘ç°å’Œèšåˆé¡¹ç›®"""

import re
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from collections import defaultdict
from loguru import logger


class ProjectDiscoveryService:
    """é¡¹ç›®å‘ç°æœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.discovered_projects = {}
        logger.info("âœ… Project Discovery Service initialized")
    
    def extract_project_names(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–é¡¹ç›®åç§°ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            é¡¹ç›®åç§°åˆ—è¡¨
        """
        projects = []
        
        # æ–¹æ³•1: æ­£åˆ™åŒ¹é…å¤§å†™å¼€å¤´çš„è¯ç»„
        pattern1 = r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b'
        matches1 = re.findall(pattern1, text)
        projects.extend(matches1)
        
        # æ–¹æ³•2: ç‰¹å®šæ¨¡å¼åŒ¹é…
        patterns = [
            r'(?:check out|introducing|new project|announcing)\s+([A-Z][a-zA-Z\s]+?)(?:\.|,|$)',
            r'([A-Z][a-zA-Z]+)\s+(?:protocol|network|chain|finance|swap)',
            r'(?:protocol|network|chain)\s+([A-Z][a-zA-Z\s]+)',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            projects.extend(matches)
        
        # æ–¹æ³•3: URLä¸­æå–
        urls = re.findall(r'https?://([a-zA-Z0-9-]+)\.[a-z]+', text)
        for url in urls:
            # è½¬æ¢åŸŸåä¸ºé¡¹ç›®å
            name = url.replace('-', ' ').title()
            if len(name) > 3:
                projects.append(name)
        
        # æ–¹æ³•4: åˆçº¦åœ°å€é™„è¿‘çš„æ–‡æœ¬
        contract_pattern = r"0x[a-fA-F0-9]{40}"
        contracts = re.findall(contract_pattern, text)
        if contracts:
            # åœ¨åˆçº¦åœ°å€å‰åæ‰¾é¡¹ç›®å
            for contract in contracts:
                idx = text.find(contract)
                before = text[max(0, idx-50):idx]
                after = text[idx:min(len(text), idx+50)]
                
                # æå–å¤§å†™å¼€å¤´çš„è¯
                nearby = before + after
                names = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', nearby)
                projects.extend(names)
        
        # æ¸…æ´—å’Œå»é‡
        projects = list(set([p.strip() for p in projects if p.strip()]))
        
        # è¿‡æ»¤å™ªéŸ³
        noise_words = {
            "The", "This", "That", "These", "Those",
            "Twitter", "Discord", "Telegram", "Medium",
            "Http", "Https", "Com", "Org", "Io",
            "New", "Launch", "Project", "Protocol",
            "And", "For", "With"
        }
        
        projects = [
            p for p in projects
            if p not in noise_words and len(p) > 2 and len(p) < 50
        ]
        
        return projects
    
    def aggregate_multi_source_data(self, data_sources: Dict[str, List[Dict]]) -> List[Dict]:
        """èšåˆå¤šå¹³å°æ•°æ®
        
        Args:
            data_sources: å„å¹³å°æ•°æ®ï¼Œæ ¼å¼ï¼š{"twitter": [...], "telegram": [...], ...}
            
        Returns:
            èšåˆåçš„é¡¹ç›®åˆ—è¡¨
        """
        logger.info("ğŸ” Aggregating multi-source data...")
        
        # é¡¹ç›®åç§° -> æåŠåˆ—è¡¨
        project_mentions = defaultdict(list)
        
        # å¤„ç†å„å¹³å°æ•°æ®
        for platform, items in data_sources.items():
            logger.info(f"  - Processing {platform}: {len(items)} items")
            
            for item in items:
                # æå–é¡¹ç›®åç§°
                text = item.get("text", "") or item.get("content", "")
                
                if not text:
                    continue
                
                projects = self.extract_project_names(text)
                
                for project in projects:
                    project_mentions[project].append({
                        "platform": platform,
                        "source_item": item,
                        "discovered_at": item.get("created_at") or item.get("date") or datetime.utcnow(),
                        "text": text[:200]  # ä¿å­˜ä¸Šä¸‹æ–‡
                    })
        
        logger.info(f"ğŸ“Š Found {len(project_mentions)} unique project names")
        
        # æ„å»ºé¡¹ç›®æ•°æ®
        aggregated_projects = []
        
        for project_name, mentions in project_mentions.items():
            # æŒ‰å¹³å°ç»Ÿè®¡
            platform_counts = defaultdict(int)
            for mention in mentions:
                platform_counts[mention["platform"]] += 1
            
            # è·¨å¹³å°ä¸€è‡´æ€§
            num_platforms = len(platform_counts)
            
            # æ€»æåŠæ•°
            total_mentions = len(mentions)
            
            # é¦–æ¬¡å‘ç°æ—¶é—´
            first_discovered = min(m["discovered_at"] for m in mentions)
            
            # æœ€è¿‘æåŠæ—¶é—´
            last_mentioned = max(m["discovered_at"] for m in mentions)
            
            # è®¡ç®—åˆæ­¥ä¿¡å·å¼ºåº¦
            signal_strength = min(100, (
                min(40, total_mentions * 5) +  # æåŠæ•°
                min(40, num_platforms * 13) +  # è·¨å¹³å°
                20  # åŸºç¡€åˆ†
            ))
            
            project_data = {
                "project_name": project_name,
                "total_mentions": total_mentions,
                "platform_mentions": dict(platform_counts),
                "num_platforms": num_platforms,
                "first_discovered_at": first_discovered,
                "last_mentioned_at": last_mentioned,
                "signal_strength": int(signal_strength),
                "mentions": mentions[:10],  # ä¿å­˜å‰10æ¡æåŠ
                "discovery_status": "new"
            }
            
            aggregated_projects.append(project_data)
        
        # æŒ‰ä¿¡å·å¼ºåº¦æ’åº
        aggregated_projects.sort(key=lambda x: x["signal_strength"], reverse=True)
        
        logger.info(f"âœ… Aggregated {len(aggregated_projects)} projects")
        return aggregated_projects
    
    def calculate_topic_heat(self, project_name: str, mentions: List[Dict]) -> Dict:
        """è®¡ç®—é¡¹ç›®è¯é¢˜çƒ­åº¦
        
        Args:
            project_name: é¡¹ç›®åç§°
            mentions: æåŠåˆ—è¡¨
            
        Returns:
            çƒ­åº¦æ•°æ®
        """
        if not mentions:
            return {"heat_score": 0}
        
        # ç»Ÿè®¡ä¸åŒæ—¶é—´æ®µçš„æåŠæ•°
        now = datetime.utcnow()
        
        # å¤„ç†æ—¥æœŸæ ¼å¼
        def parse_date(d):
            if isinstance(d, datetime):
                return d
            elif isinstance(d, str):
                try:
                    return datetime.fromisoformat(d.replace('Z', '+00:00'))
                except:
                    return datetime.utcnow()
            else:
                return datetime.utcnow()
        
        mentions_24h = [m for m in mentions if (now - parse_date(m["discovered_at"])).total_seconds() < 86400]
        mentions_7d = [m for m in mentions if (now - parse_date(m["discovered_at"])).total_seconds() < 604800]
        
        # è®¡ç®—å¢é•¿ç‡
        if len(mentions_7d) > len(mentions_24h):
            recent_rate = len(mentions_24h) / (len(mentions_7d) - len(mentions_24h))
        else:
            recent_rate = 1.0
        
        # è®¡ç®—çƒ­åº¦åˆ†
        heat_score = min(100, (
            min(50, len(mentions_24h) * 10) +  # 24å°æ—¶æåŠ
            min(30, recent_rate * 30) +  # å¢é•¿ç‡
            min(20, len(set(m["platform"] for m in mentions_24h)) * 7)  # å¹³å°æ•°
        ))
        
        return {
            "project_name": project_name,
            "heat_score": int(heat_score),
            "mentions_24h": len(mentions_24h),
            "mentions_7d": len(mentions_7d),
            "growth_rate": recent_rate,
            "is_trending": heat_score > 70
        }
    
    def detect_sudden_surge(self, project_name: str, mentions: List[Dict]) -> Dict:
        """æ£€æµ‹çªç„¶çˆ†å‘
        
        Args:
            project_name: é¡¹ç›®åç§°
            mentions: æåŠåˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
            
        Returns:
            çˆ†å‘æ£€æµ‹ç»“æœ
        """
        if len(mentions) < 10:
            return {"is_surge": False}
        
        # æŒ‰å°æ—¶ç»Ÿè®¡
        hourly_counts = defaultdict(int)
        
        for mention in mentions:
            hour = mention["discovered_at"].replace(minute=0, second=0, microsecond=0)
            hourly_counts[hour] += 1
        
        # è½¬æ¢ä¸ºåˆ—è¡¨
        sorted_hours = sorted(hourly_counts.keys())
        counts = [hourly_counts[h] for h in sorted_hours]
        
        if len(counts) < 24:
            return {"is_surge": False}
        
        # è®¡ç®—åŸºçº¿ï¼ˆå‰é¢çš„å¹³å‡å€¼ï¼‰
        baseline = sum(counts[:-24]) / len(counts[:-24]) if len(counts) > 24 else 0
        
        # æœ€è¿‘24å°æ—¶å¹³å‡
        recent_avg = sum(counts[-24:]) / 24
        
        # åˆ¤æ–­çˆ†å‘
        surge_ratio = recent_avg / baseline if baseline > 0 else 0
        is_surge = surge_ratio > 3
        
        return {
            "project_name": project_name,
            "is_surge": is_surge,
            "surge_ratio": surge_ratio,
            "baseline_per_hour": baseline,
            "recent_per_hour": recent_avg
        }
    
    def filter_known_projects(self, projects: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤å·²çŸ¥å¤§é¡¹ç›®
        
        Args:
            projects: é¡¹ç›®åˆ—è¡¨
            
        Returns:
            è¿‡æ»¤åçš„é¡¹ç›®åˆ—è¡¨
        """
        # å·²çŸ¥å¤§é¡¹ç›®åˆ—è¡¨
        known_big_projects = {
            "Bitcoin", "Ethereum", "Binance", "Coinbase",
            "Uniswap", "Aave", "Compound", "Maker",
            "Polygon", "Solana", "Avalanche", "Cardano",
            "Polkadot", "Chainlink", "Cosmos", "Tezos"
        }
        
        filtered = [
            p for p in projects
            if p["project_name"] not in known_big_projects
        ]
        
        logger.info(f"ğŸ“Š Filtered out {len(projects) - len(filtered)} known big projects")
        return filtered
    
    def check_token_status(self, project_name: str) -> Dict:
        """æ£€æŸ¥é¡¹ç›®æ˜¯å¦å·²å‘å¸ï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        Args:
            project_name: é¡¹ç›®åç§°
            
        Returns:
            ä»£å¸çŠ¶æ€
        """
        # TODO: é›†æˆCoinGecko/CoinMarketCap API
        
        # ç®€å•çš„å…³é”®è¯æ£€æµ‹
        no_token_keywords = ["testnet", "coming soon", "pre-launch"]
        has_token_keywords = ["token", "launched", "live"]
        
        # è¿™é‡Œè¿”å›å‡è®¾å€¼ï¼Œå®é™…éœ€è¦è°ƒç”¨API
        return {
            "project_name": project_name,
            "has_token": False,  # å‡è®¾æœªå‘å¸
            "checked_at": datetime.utcnow()
        }
    
    def discover_projects(self, data_sources: Dict[str, List[Dict]]) -> List[Dict]:
        """å®Œæ•´çš„é¡¹ç›®å‘ç°æµç¨‹
        
        Args:
            data_sources: å„å¹³å°æ•°æ®
            
        Returns:
            å‘ç°çš„é¡¹ç›®åˆ—è¡¨
        """
        logger.info("ğŸš€ Starting project discovery process...")
        
        # 1. èšåˆå¤šå¹³å°æ•°æ®
        aggregated = self.aggregate_multi_source_data(data_sources)
        logger.info(f"  âœ… Step 1: Aggregated {len(aggregated)} projects")
        
        # 2. è¿‡æ»¤å·²çŸ¥å¤§é¡¹ç›®
        filtered = self.filter_known_projects(aggregated)
        logger.info(f"  âœ… Step 2: Filtered to {len(filtered)} projects")
        
        # 3. è®¡ç®—çƒ­åº¦å’Œçˆ†å‘
        for project in filtered:
            # è¯é¢˜çƒ­åº¦
            heat = self.calculate_topic_heat(
                project["project_name"],
                project["mentions"]
            )
            project["heat_data"] = heat
            
            # çªå‘æ£€æµ‹
            surge = self.detect_sudden_surge(
                project["project_name"],
                project["mentions"]
            )
            project["surge_data"] = surge
            
            # æ£€æŸ¥ä»£å¸çŠ¶æ€
            token_status = self.check_token_status(project["project_name"])
            project["token_status"] = token_status
        
        logger.info("  âœ… Step 3: Calculated heat and surge data")
        
        # 4. ç­›é€‰é«˜è´¨é‡é¡¹ç›®
        high_quality = [
            p for p in filtered
            if (
                p["num_platforms"] >= 2 and  # è‡³å°‘2ä¸ªå¹³å°
                p["total_mentions"] >= 3 and  # è‡³å°‘3æ¬¡æåŠ
                not p["token_status"]["has_token"]  # æœªå‘å¸
            )
        ]
        
        logger.info(f"  âœ… Step 4: {len(high_quality)} high-quality projects")
        
        # 5. æŒ‰ç»¼åˆä¿¡å·æ’åº
        high_quality.sort(
            key=lambda x: (
                x["heat_data"].get("heat_score", 0) * 0.6 +
                x["signal_strength"] * 0.4
            ),
            reverse=True
        )
        
        logger.info(f"âœ… Discovery complete: {len(high_quality)} projects found")
        
        return high_quality


# å…¨å±€æœåŠ¡å®ä¾‹
project_discovery_service = ProjectDiscoveryService()

