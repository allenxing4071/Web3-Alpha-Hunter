# 多平台数据采集架构设计

## 📋 文档概述

**文档目标**: 详细设计从多个平台采集Web3项目信息的技术方案  
**核心理念**: 不同平台特性不同，需要定制化的采集策略  
**更新时间**: 2025-10-04

---

## 🗺️ 整体架构

```
┌─────────────────────────────────────────────────────────────┐
│           多平台数据采集总控 (Multi-Source Orchestrator)      │
│                                                               │
│  - 任务调度（Celery）                                         │
│  - 优先级队列管理                                             │
│  - 错误重试机制                                               │
│  - 数据去重与归一化                                           │
└─────────────────────────────────────────────────────────────┘
                              │
        ┌─────────────────────┼─────────────────────┐
        │                     │                     │
   文字平台              社交平台              视频平台
        │                     │                     │
┌───────┴────────┐   ┌────────┴────────┐   ┌───────┴────────┐
│ Twitter/X      │   │ Telegram        │   │ YouTube        │
│ Medium         │   │ Discord         │   │ TikTok         │
│ Mirror.xyz     │   │ Reddit          │   │ Bilibili       │
│ Substack       │   │ Farcaster       │   │                │
└────────────────┘   └─────────────────┘   └────────────────┘
```

---

## 📊 平台优先级矩阵

### 第一梯队（优先实现 - Week 1-2）

| 平台 | 类型 | 优先级 | 日均信息量 | 实时性 | 技术难度 |
|------|------|--------|-----------|--------|----------|
| **Twitter/X** | 社交 | ⭐⭐⭐⭐⭐ | 5000+ 推文 | 实时 | 中等 |
| **Telegram** | 社交 | ⭐⭐⭐⭐⭐ | 3000+ 消息 | 实时 | 中等 |
| **Discord** | 社交 | ⭐⭐⭐⭐ | 2000+ 消息 | 实时 | 较高 |
| **Medium** | 文字 | ⭐⭐⭐⭐ | 200+ 文章 | 小时级 | 低 |

### 第二梯队（Week 3-4）

| 平台 | 类型 | 优先级 | 日均信息量 | 实时性 | 技术难度 |
|------|------|--------|-----------|--------|----------|
| **Reddit** | 社交 | ⭐⭐⭐ | 1000+ 帖子 | 小时级 | 低 |
| **YouTube** | 视频 | ⭐⭐⭐ | 50+ 视频 | 天级 | 中等 |
| **Mirror.xyz** | 文字 | ⭐⭐⭐ | 50+ 文章 | 天级 | 低 |

### 第三梯队（Week 5+）

| 平台 | 类型 | 优先级 | 日均信息量 | 实时性 | 技术难度 |
|------|------|--------|-----------|--------|----------|
| **抖音/TikTok** | 视频 | ⭐⭐ | 500+ 视频 | 实时 | 高 |
| **小红书** | 图文 | ⭐⭐ | 300+ 笔记 | 小时级 | 中等 |
| **B站** | 视频 | ⭐⭐ | 100+ 视频 | 天级 | 中等 |
| **Farcaster** | 社交 | ⭐⭐ | 200+ 消息 | 实时 | 中等 |

---

## 🐦 平台一: Twitter/X

### 1.1 平台特点

**为什么重要**:
- ✅ Web3信息传播最快的平台
- ✅ KOL密集，意见领袖影响力大
- ✅ 项目官方首发地
- ✅ 话题热度最容易观察

**数据特点**:
- 短文本（280字符）
- 高互动性（转发、点赞、评论）
- 实时性强（秒级更新）
- 社交图谱明确

### 1.2 采集策略

#### 策略一：关键词监控

**目标**: 捕获所有提及未发币项目的推文

**关键词库**:
```python
KEYWORDS = {
    # 项目阶段
    "presale", "fair launch", "stealth launch", "soft launch",
    "testnet", "mainnet launch", "beta launch",
    
    # 融资相关
    "seed round", "series A", "raised $", "funding announcement",
    "backed by", "investment from",
    
    # 代币相关
    "token launch", "TGE", "IDO", "ICO", "IEO", "airdrop",
    "token sale", "whitelist", "allocation",
    
    # 项目特征
    "no token yet", "未发币", "pre-token", "points system",
    "early stage", "stealth mode",
    
    # 技术标签
    "DeFi", "NFT", "GameFi", "SocialFi", "Layer2", "zkSync",
    "Optimistic Rollup", "Zero Knowledge",
}
```

**查询构建**:
```python
# 示例查询
query = """
(presale OR "fair launch" OR "no token yet" OR airdrop) 
(DeFi OR NFT OR GameFi OR Layer2)
-is:retweet 
lang:en
"""

# 高级筛选
filters = {
    "min_followers": 1000,      # 发推人最低粉丝数
    "min_engagement": 10,        # 最低互动数（转发+点赞）
    "exclude_verified_only": False,  # 不排除未认证账号
}
```

#### 策略二：KOL追踪

**目标**: 监控顶级加密KOL的每条推文和互动

**KOL分级体系**:

```python
TIER_1_KOLS = [
    # 行业领袖（粉丝>1M）
    "VitalikButerin",        # 以太坊创始人
    "cz_binance",            # Binance创始人
    "brian_armstrong",       # Coinbase CEO
    "SBF_FTX",               # FTX创始人（历史参考）
    
    # 顶级投资人（粉丝>500K）
    "APompliano",            # Pomp
    "RaoulGMI",              # Real Vision
    "novogratz",             # Galaxy Digital
]

TIER_2_KOLS = [
    # 资深分析师（粉丝>100K）
    "TheCryptoLark",
    "CryptoCred",
    "inversebrah",
    "CryptoWendyO",
    "IamCryptoWolf",
    
    # 技术专家
    "hasufl",                # MEV专家
    "0xMaki",                # DeFi
    "DCinvestor",            # 以太坊
]

TIER_3_KOLS = [
    # 新兴KOL（粉丝>10K）
    "0xSisyphus",
    "Dynamo_Patrick",
    "DefiIgnas",
    # ... 100+ KOLs
]
```

**追踪逻辑**:
```python
def track_kol_activity(username: str, tier: int):
    """追踪KOL活动"""
    
    # 1. 获取原创推文（排除转发）
    tweets = get_user_tweets(
        username=username,
        exclude=["retweets"],
        max_results=10
    )
    
    # 2. 获取点赞的推文（发现关注点）
    liked_tweets = get_user_likes(
        username=username,
        max_results=20
    )
    
    # 3. 获取转发的推文（强烈推荐信号）
    retweeted = get_user_retweets(
        username=username,
        max_results=20
    )
    
    # 权重计算
    for tweet in tweets:
        tweet["signal_strength"] = calculate_signal(
            is_original=True,
            author_tier=tier,
            engagement=tweet["engagement"]
        )
    
    return {
        "original": tweets,
        "liked": liked_tweets,
        "retweeted": retweeted
    }
```

#### 策略三：评论区挖掘（核心！）

**目标**: 从热门推文的评论区发现早期项目讨论

**为什么重要**:
- 评论区往往有"知情者"提前透露信息
- 社区讨论的项目往往是真实需求
- 评论区互动数据反映真实热度

**挖掘算法**:
```python
def mine_comment_section(tweet_id: str):
    """挖掘评论区"""
    
    # 1. 获取所有评论
    comments = get_tweet_replies(
        tweet_id=tweet_id,
        max_results=100,
        sort_by="engagement"  # 按互动数排序
    )
    
    # 2. 评论质量过滤
    quality_comments = []
    for comment in comments:
        score = calculate_comment_quality(comment)
        if score > 60:  # 质量阈值
            quality_comments.append(comment)
    
    # 3. 项目提及提取
    mentioned_projects = []
    for comment in quality_comments:
        projects = extract_project_mentions(comment["text"])
        
        for project in projects:
            mentioned_projects.append({
                "project_name": project,
                "mentioned_by": comment["author"],
                "author_followers": comment["author_followers"],
                "comment_likes": comment["likes"],
                "sentiment": analyze_sentiment(comment["text"]),
                "context": comment["text"][:200]
            })
    
    # 4. 项目热度聚合
    project_heatmap = aggregate_project_mentions(mentioned_projects)
    
    return project_heatmap


def calculate_comment_quality(comment: dict) -> int:
    """计算评论质量分（0-100）"""
    
    score = 0
    
    # 因子1: 评论者粉丝数（30分）
    followers = comment["author_followers"]
    score += min(30, followers / 1000)
    
    # 因子2: 评论互动数（30分）
    engagement = comment["likes"] + comment["replies"]
    score += min(30, engagement / 10)
    
    # 因子3: 评论长度（20分）- 长评论往往更有价值
    text_length = len(comment["text"])
    if 50 < text_length < 280:
        score += 20
    elif text_length >= 280:
        score += 15
    
    # 因子4: 包含链接或合约地址（20分）
    has_url = "http" in comment["text"]
    has_contract = re.match(r"0x[a-fA-F0-9]{40}", comment["text"])
    if has_url or has_contract:
        score += 20
    
    # 惩罚项: 垃圾评论特征
    spam_keywords = ["follow me", "dm me", "check my profile", "100x guaranteed"]
    for keyword in spam_keywords:
        if keyword.lower() in comment["text"].lower():
            score -= 30
    
    return max(0, min(100, score))
```

#### 策略四：话题热度追踪

**目标**: 发现突然爆发的话题和hashtag

**热度计算公式**:
```python
def calculate_topic_heat(topic: str, time_window: int = 24) -> dict:
    """计算话题热度"""
    
    # 1. 获取时间窗口内的提及数据
    mentions = get_topic_mentions(
        topic=topic,
        hours=time_window
    )
    
    # 2. 分时段统计
    hourly_counts = group_by_hour(mentions)
    
    # 3. 计算增长率
    recent_hour = hourly_counts[-1]
    previous_avg = sum(hourly_counts[:-1]) / len(hourly_counts[:-1])
    growth_rate = (recent_hour - previous_avg) / previous_avg if previous_avg > 0 else 0
    
    # 4. 计算KOL参与度
    kol_participation = count_kol_mentions(mentions)
    
    # 5. 综合热度分
    heat_score = (
        min(50, recent_hour) +                    # 绝对数量（最高50分）
        min(30, growth_rate * 100) +              # 增长率（最高30分）
        min(20, kol_participation * 4)            # KOL参与（最高20分）
    )
    
    return {
        "topic": topic,
        "heat_score": heat_score,
        "hourly_mentions": hourly_counts,
        "growth_rate": f"{growth_rate*100:.1f}%",
        "kol_count": kol_participation,
        "is_trending": heat_score > 70
    }
```

### 1.3 数据结构设计

```python
class TwitterData(BaseModel):
    """Twitter采集数据模型"""
    
    # 基础信息
    tweet_id: str
    author_id: str
    author_username: str
    author_display_name: str
    
    # 账号属性
    author_verified: bool
    author_followers: int
    author_following: int
    author_created_at: datetime
    
    # 推文内容
    text: str
    language: str
    created_at: datetime
    
    # 互动数据
    likes: int
    retweets: int
    replies: int
    quotes: int
    bookmarks: int
    impressions: Optional[int]
    
    # 实体提取
    urls: List[str]
    hashtags: List[str]
    mentions: List[str]
    cashtags: List[str]  # $BTC, $ETH
    
    # 媒体
    has_media: bool
    media_types: List[str]  # photo, video, gif
    
    # 元数据
    is_reply: bool
    is_quote: bool
    replied_to_id: Optional[str]
    quoted_tweet_id: Optional[str]
    
    # 分析结果
    matched_keywords: List[str]
    mentioned_projects: List[str]
    contracts_found: List[str]
    sentiment_score: Optional[float]  # -1到1
    
    # 信号强度
    signal_strength: int  # 0-100
    is_from_kol: bool
    kol_tier: Optional[int]
    
    # 采集元信息
    collected_at: datetime
    collection_method: str  # keyword_search, kol_tracking, comment_mining


class TwitterCommentData(BaseModel):
    """评论区数据模型"""
    
    parent_tweet_id: str
    comment_id: str
    
    # 评论者信息
    author_username: str
    author_followers: int
    
    # 评论内容
    text: str
    created_at: datetime
    
    # 互动数据
    likes: int
    replies: int
    
    # 提及的项目
    mentioned_projects: List[ProjectMention]
    
    # 质量评分
    quality_score: int  # 0-100
    
    # 情感
    sentiment: str  # positive, neutral, negative


class ProjectMention(BaseModel):
    """项目提及"""
    
    project_name: str
    confidence: float  # 0-1，识别置信度
    context: str  # 提及时的上下文
    sentiment: str
    mentioned_by: str
    mention_source: str  # tweet, comment, kol_post
```

### 1.4 技术实现

#### API选择

**官方API（推荐）**:
```python
import tweepy

# 初始化客户端
client = tweepy.Client(
    bearer_token=TWITTER_BEARER_TOKEN,
    consumer_key=TWITTER_API_KEY,
    consumer_secret=TWITTER_API_SECRET,
    access_token=TWITTER_ACCESS_TOKEN,
    access_token_secret=TWITTER_ACCESS_SECRET,
    wait_on_rate_limit=True  # 自动处理限流
)

# API限制（V2 API）
RATE_LIMITS = {
    "search_recent": "450次/15分钟",
    "user_tweets": "1500次/15分钟",
    "tweet_lookup": "300次/15分钟",
}
```

**备用方案（Twitter Scraper）**:
```python
from snscrape.modules.twitter import TwitterSearchScraper

# 无需API密钥，但不稳定
scraper = TwitterSearchScraper('crypto since:2025-01-01')
for tweet in scraper.get_items():
    process_tweet(tweet)
```

#### 反爬虫策略

```python
# 策略1: 多账号轮换
accounts = [
    {"bearer_token": TOKEN_1, "last_used": None},
    {"bearer_token": TOKEN_2, "last_used": None},
    {"bearer_token": TOKEN_3, "last_used": None},
]

def get_available_client():
    """获取可用客户端"""
    for account in accounts:
        if can_use(account):
            return tweepy.Client(bearer_token=account["bearer_token"])
    
    # 全部限流，等待
    time.sleep(60)
    return get_available_client()


# 策略2: 请求间隔
import time
import random

def rate_limited_request(func, *args, **kwargs):
    """限流请求"""
    time.sleep(random.uniform(1, 3))  # 随机延迟1-3秒
    return func(*args, **kwargs)


# 策略3: 错误重试
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
def robust_api_call(client, method, **params):
    """健壮的API调用"""
    try:
        return method(**params)
    except tweepy.TooManyRequests:
        logger.warning("Rate limit hit, waiting...")
        time.sleep(900)  # 等15分钟
        raise
    except tweepy.TwitterServerError:
        logger.error("Twitter server error")
        raise
```

#### 采集任务调度

```python
# Celery定时任务
from celery import Celery
from celery.schedules import crontab

app = Celery('twitter_collector')

@app.task
def collect_keyword_tweets():
    """采集关键词推文 - 每5分钟"""
    collector = TwitterCollector()
    tweets = collector.monitor_keywords(hours=1)
    save_to_database(tweets)


@app.task
def collect_kol_tweets():
    """采集KOL推文 - 每15分钟"""
    collector = TwitterCollector()
    
    # Tier1 KOL - 每次采集
    for kol in TIER_1_KOLS:
        tweets = collector.track_kol_tweets(kol, max_results=10)
        save_to_database(tweets)
    
    # Tier2/3 KOL - 轮询采集
    kol_sample = sample_kols(TIER_2_KOLS + TIER_3_KOLS, size=20)
    for kol in kol_sample:
        tweets = collector.track_kol_tweets(kol, max_results=5)
        save_to_database(tweets)


@app.task
def mine_comment_sections():
    """挖掘评论区 - 每小时"""
    
    # 找到过去1小时内高互动推文
    hot_tweets = get_hot_tweets(
        min_engagement=100,
        hours=1
    )
    
    # 挖掘每条推文的评论区
    for tweet in hot_tweets:
        comments = mine_comment_section(tweet["tweet_id"])
        save_to_database(comments)


@app.task
def analyze_trending_topics():
    """分析热门话题 - 每2小时"""
    
    # 获取Twitter Trending Topics
    trends = get_twitter_trends(location="Worldwide")
    
    # 筛选加密相关话题
    crypto_trends = filter_crypto_trends(trends)
    
    # 计算每个话题的热度
    for topic in crypto_trends:
        heat_data = calculate_topic_heat(topic)
        if heat_data["is_trending"]:
            send_alert(f"🔥 新热点: {topic}")


# 任务调度配置
app.conf.beat_schedule = {
    'collect-keywords': {
        'task': 'collect_keyword_tweets',
        'schedule': 300.0,  # 每5分钟
    },
    'collect-kols': {
        'task': 'collect_kol_tweets',
        'schedule': 900.0,  # 每15分钟
    },
    'mine-comments': {
        'task': 'mine_comment_sections',
        'schedule': 3600.0,  # 每小时
    },
    'analyze-trends': {
        'task': 'analyze_trending_topics',
        'schedule': 7200.0,  # 每2小时
    },
}
```

### 1.5 数据质量保证

```python
def validate_twitter_data(data: TwitterData) -> bool:
    """验证数据质量"""
    
    # 检查1: 必填字段
    required_fields = ["tweet_id", "author_username", "text", "created_at"]
    for field in required_fields:
        if not getattr(data, field):
            return False
    
    # 检查2: 去重
    if is_duplicate(data.tweet_id):
        return False
    
    # 检查3: 时效性（不接受超过24小时的数据）
    age = datetime.now() - data.created_at
    if age.total_seconds() > 86400:
        return False
    
    # 检查4: 垃圾过滤
    if is_spam(data.text):
        return False
    
    return True


def is_spam(text: str) -> bool:
    """垃圾推文检测"""
    
    spam_patterns = [
        r"(follow|like|retweet) for (follow|like|retweet)",
        r"dm me for",
        r"100x guaranteed",
        r"get rich quick",
        r"pump (now|tonight|tomorrow)",
    ]
    
    for pattern in spam_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    
    return False
```

### 1.6 验收标准

**功能验收**:
- ✅ 能够每5分钟采集关键词相关推文
- ✅ 能够追踪100+ KOL的最新推文
- ✅ 能够挖掘热门推文的评论区
- ✅ 能够识别突然爆发的话题

**性能验收**:
- ✅ 日采集量 > 5,000条推文
- ✅ 数据延迟 < 10分钟
- ✅ 去重准确率 > 95%
- ✅ API限流处理成功率 > 99%

**质量验收**:
- ✅ 垃圾推文过滤率 > 90%
- ✅ 项目名称提取准确率 > 80%
- ✅ 情感分析准确率 > 75%

---

## 💬 平台二: Telegram

### 2.1 平台特点

**为什么重要**:
- ✅ 项目官方首发渠道（超过90%的项目有Telegram群）
- ✅ 社区讨论最活跃
- ✅ 官方公告最及时
- ✅ AMA、白名单等活动发布地

**数据特点**:
- 群组/频道二分
- 支持机器人
- 消息量巨大
- 隐私保护强

### 2.2 采集策略

#### 策略一：频道订阅

**目标**: 监控所有主流加密货币频道

**频道分类**:
```python
CHANNELS = {
    "news": [
        "@coindesk",
        "@cointelegraph",
        "@theblockofficial",
        "@cryptonewsflash",
    ],
    
    "announcements": [
        "@binance_announcements",
        "@coinbase_official",
        "@uniswap_announcements",
        "@ethereum_announcements",
    ],
    
    "alpha": [
        "@cryptogemhunters",
        "@defiprojects",
        "@nftdrops_official",
        "@airdropalerts",
    ],
    
    "kol": [
        "@VitalikChannel",
        "@CZofficialEN",
        "@AltcoinGordon",
    ],
    
    "vc": [
        "@a16zcrypto",
        "@paradigmxyz",
        "@binancelabs",
    ]
}
```

#### 策略二：群组监控

**目标**: 加入并监控活跃的加密货币群组

**群组发现**:
```python
def discover_crypto_groups():
    """发现新的加密货币群组"""
    
    # 方法1: 从现有群组的讨论中提取
    for group in monitored_groups:
        messages = get_recent_messages(group, limit=100)
        
        for msg in messages:
            # 提取 t.me/ 链接
            invite_links = re.findall(r't\.me/([a-zA-Z0-9_]+)', msg.text)
            
            for link in invite_links:
                if is_crypto_related(link):
                    add_to_monitor_list(link)
    
    # 方法2: 从Twitter推文中提取
    tweets = search_tweets("telegram.me OR t.me crypto OR web3")
    for tweet in tweets:
        extract_telegram_links(tweet)
    
    # 方法3: 爬取Telegram群组目录网站
    directories = [
        "https://telegramchannels.me/crypto",
        "https://tlgrm.eu/channels/crypto",
    ]
```

**群组质量评估**:
```python
def assess_group_quality(group_username: str) -> int:
    """评估群组质量（0-100分）"""
    
    info = get_group_info(group_username)
    messages = get_recent_messages(group_username, limit=100)
    
    score = 0
    
    # 成员数（30分）
    members = info["member_count"]
    score += min(30, members / 1000)
    
    # 活跃度（30分）
    daily_messages = len([m for m in messages if is_within_24h(m)])
    score += min(30, daily_messages / 10)
    
    # 内容质量（20分）
    spam_ratio = count_spam(messages) / len(messages)
    score += max(0, 20 * (1 - spam_ratio))
    
    # 官方认证（20分）
    if info.get("verified"):
        score += 20
    
    return int(score)
```

#### 策略三：关键信息捕获

**目标**: 从海量消息中提取关键信息

**信息类型**:
```python
class KeyInformation:
    """关键信息类型"""
    
    PROJECT_ANNOUNCEMENT = "项目公告"        # 新项目发布
    FUNDING_NEWS = "融资消息"                # 完成融资
    TOKEN_LAUNCH = "代币上线"                # TGE/IDO
    AIRDROP_ALERT = "空投通知"               # 空投活动
    WHITELIST_OPEN = "白名单开放"            # 白名单
    TESTNET_LAUNCH = "测试网上线"            # 测试网
    MAINNET_LAUNCH = "主网上线"              # 主网
    PARTNERSHIP = "合作伙伴"                 # 合作公告
    AMA_SCHEDULE = "AMA安排"                 # AMA活动
    CONTRACT_ADDRESS = "合约地址"            # 合约发布


def extract_key_information(message: dict) -> Optional[dict]:
    """提取关键信息"""
    
    text = message["text"].lower()
    
    # 检测信息类型
    info_type = None
    
    if any(kw in text for kw in ["announcing", "introduce", "launch"]):
        info_type = KeyInformation.PROJECT_ANNOUNCEMENT
    
    elif any(kw in text for kw in ["raised", "funding", "investment", "seed", "series"]):
        info_type = KeyInformation.FUNDING_NEWS
        # 提取金额
        amount = extract_funding_amount(text)
    
    elif any(kw in text for kw in ["airdrop", "free tokens", "claim"]):
        info_type = KeyInformation.AIRDROP_ALERT
    
    elif any(kw in text for kw in ["whitelist", "early access", "allowlist"]):
        info_type = KeyInformation.WHITELIST_OPEN
    
    elif re.search(r"0x[a-fA-F0-9]{40}", text):
        info_type = KeyInformation.CONTRACT_ADDRESS
        contract = extract_contract_address(text)
    
    # 如果识别到关键信息
    if info_type:
        return {
            "type": info_type,
            "channel": message["channel"],
            "text": message["text"],
            "timestamp": message["date"],
            "importance": calculate_importance(info_type, message)
        }
    
    return None
```

#### 策略四：官方群验证

**目标**: 区分官方群和山寨群

```python
def verify_official_group(group_username: str, project_name: str) -> bool:
    """验证是否为官方群"""
    
    # 验证1: 检查项目官网
    official_links = get_official_links_from_website(project_name)
    if f"t.me/{group_username}" in official_links:
        return True
    
    # 验证2: 检查Twitter认证
    twitter_bio = get_twitter_bio(project_name)
    if group_username in twitter_bio:
        return True
    
    # 验证3: 检查群组描述和管理员
    group_info = get_group_info(group_username)
    
    # 官方群特征
    has_project_name = project_name.lower() in group_info["description"].lower()
    has_official_admin = any(
        admin["username"] in KNOWN_TEAM_MEMBERS[project_name]
        for admin in group_info["admins"]
    )
    
    if has_project_name and has_official_admin:
        return True
    
    # 验证4: 交叉验证
    # 真正的官方群通常会在多个地方被提及
    mentions = count_official_mentions(group_username, project_name)
    if mentions > 3:
        return True
    
    return False
```

### 2.3 数据结构设计

```python
class TelegramData(BaseModel):
    """Telegram采集数据模型"""
    
    # 基础信息
    message_id: int
    chat_id: int
    chat_username: Optional[str]
    chat_title: str
    chat_type: str  # channel, group, supergroup
    
    # 发送者信息
    sender_id: Optional[int]
    sender_username: Optional[str]
    sender_first_name: Optional[str]
    is_bot: bool
    
    # 消息内容
    text: str
    date: datetime
    edit_date: Optional[datetime]
    
    # 互动数据
    views: Optional[int]  # 仅频道有
    forwards: Optional[int]
    replies: Optional[int]
    
    # 媒体
    has_media: bool
    media_type: Optional[str]  # photo, video, document, audio
    
    # 实体提取
    urls: List[str]
    mentions: List[str]
    hashtags: List[str]
    cashtags: List[str]
    
    # 特殊内容
    contracts: List[str]  # 0x...
    telegram_links: List[str]  # t.me/...
    
    # 分析结果
    key_information_type: Optional[str]
    mentioned_projects: List[str]
    importance_score: int  # 0-100
    
    # 验证信息
    is_official_channel: bool
    channel_quality_score: int  # 0-100
    
    # 采集元信息
    collected_at: datetime
    collection_method: str


class TelegramChannelInfo(BaseModel):
    """Telegram频道/群组信息"""
    
    username: str
    title: str
    description: str
    
    member_count: int
    online_count: Optional[int]
    
    is_verified: bool
    is_scam: bool
    is_fake: bool
    
    created_date: Optional[datetime]
    
    # 关联项目
    related_project: Optional[str]
    is_official: bool
    
    # 质量评分
    quality_score: int
    activity_level: str  # high, medium, low
    
    # 统计数据
    daily_message_count: int
    spam_ratio: float
    
    last_checked: datetime
```

### 2.4 技术实现

```python
from telethon import TelegramClient, events
from telethon.tl.types import Channel, MessageMediaDocument

# 初始化客户端
client = TelegramClient(
    'web3_alpha_hunter',
    api_id=TELEGRAM_API_ID,
    api_hash=TELEGRAM_API_HASH
)

# 启动客户端
await client.start()


# 方法1: 获取历史消息
async def collect_channel_history(channel_username: str, limit: int = 100):
    """采集频道历史消息"""
    
    entity = await client.get_entity(channel_username)
    
    messages = []
    async for message in client.iter_messages(entity, limit=limit):
        if message.text:
            messages.append({
                "message_id": message.id,
                "text": message.text,
                "date": message.date,
                "views": message.views,
                "forwards": message.forwards,
            })
    
    return messages


# 方法2: 实时监听（推荐）
@client.on(events.NewMessage(chats=MONITORED_CHANNELS))
async def handle_new_message(event):
    """处理新消息"""
    
    message = event.message
    
    # 提取关键信息
    key_info = extract_key_information({
        "text": message.text,
        "date": message.date,
        "channel": event.chat.username
    })
    
    # 如果是重要信息，立即处理
    if key_info and key_info["importance"] > 80:
        await send_urgent_alert(key_info)
        await save_to_database(key_info)
    
    # 记录所有消息
    await save_message_to_database(message)


# 方法3: 批量监控
async def monitor_all_channels():
    """监控所有频道"""
    
    # 启动实时监听
    await client.start()
    
    logger.info(f"开始监控 {len(MONITORED_CHANNELS)} 个频道")
    
    # 保持运行
    await client.run_until_disconnected()


# 频道发现与加入
async def join_channel(channel_username: str):
    """加入频道/群组"""
    
    try:
        await client(JoinChannelRequest(channel_username))
        logger.info(f"✅ 已加入 {channel_username}")
        return True
    except Exception as e:
        logger.error(f"❌ 加入 {channel_username} 失败: {e}")
        return False
```

### 2.5 反封禁策略

```python
# 策略1: 多账号轮换
telegram_accounts = [
    {"phone": "+1234567890", "api_id": ID1, "api_hash": HASH1},
    {"phone": "+0987654321", "api_id": ID2, "api_hash": HASH2},
]

# 策略2: 行为模拟
async def human_like_behavior():
    """模拟人类行为"""
    
    # 随机延迟
    await asyncio.sleep(random.uniform(5, 15))
    
    # 不要一次性加入太多群组
    max_joins_per_day = 10
    
    # 定期互动（点赞、回复）
    await random_interaction()


# 策略3: 避免触发反垃圾机制
RATE_LIMITS = {
    "join_channel": "10次/天",
    "send_message": "20次/分钟",
    "forward_message": "30次/分钟",
}
```

### 2.6 验收标准

**功能验收**:
- ✅ 能够监控 100+ 频道/群组
- ✅ 能够实时接收新消息（延迟<30秒）
- ✅ 能够自动加入新发现的群组
- ✅ 能够识别官方群 vs 山寨群

**性能验收**:
- ✅ 日采集量 > 3,000条消息
- ✅ 官方公告捕获率 > 95%
- ✅ 合约地址识别准确率 > 98%

**质量验收**:
- ✅ 垃圾消息过滤率 > 85%
- ✅ 关键信息提取准确率 > 90%
- ✅ 官方群验证准确率 > 95%

---

## 🎮 平台三: Discord

### 3.1 平台特点

**为什么重要**:
- ✅ Web3项目的"总部"（开发者、核心社区聚集地）
- ✅ 深度讨论、技术交流
- ✅ 官方AMA、白名单分发
- ✅ 社区活跃度最高

**数据特点**:
- 服务器-频道结构
- 实时性强
- 互动复杂（回复、表情、投票）
- 权限管理严格

### 3.2 采集策略

#### 策略一：服务器发现与加入

**目标**: 加入所有主流Web3项目的Discord服务器

**服务器分类**:
```python
DISCORD_SERVERS = {
    "layer1": [
        {"name": "Ethereum", "invite": "ethereum-org"},
        {"name": "Solana", "invite": "solana"},
        {"name": "Avalanche", "invite": "avalanche"},
    ],
    
    "defi": [
        {"name": "Uniswap", "invite": "uniswap"},
        {"name": "Aave", "invite": "aave"},
        {"name": "Curve", "invite": "curve"},
    ],
    
    "nft": [
        {"name": "OpenSea", "invite": "opensea"},
        {"name": "Blur", "invite": "blur"},
    ],
    
    "emerging": [
        # 新兴项目服务器（动态更新）
    ]
}
```

**服务器发现**:
```python
def discover_discord_servers():
    """发现新的Discord服务器"""
    
    # 方法1: 从Twitter/Telegram提取
    invite_links = []
    
    tweets = search_tweets("discord.gg OR discord.com/invite")
    for tweet in tweets:
        links = re.findall(r'discord\.gg/([a-zA-Z0-9-]+)', tweet.text)
        invite_links.extend(links)
    
    # 方法2: 从项目官网提取
    for project in discovered_projects:
        website_content = fetch_website(project.website)
        discord_link = extract_discord_link(website_content)
        if discord_link:
            invite_links.append(discord_link)
    
    # 方法3: 从Discord服务器列表网站爬取
    directories = [
        "https://disboard.org/servers/tag/crypto",
        "https://top.gg/servers/tag/crypto",
    ]
    
    return deduplicate(invite_links)
```

#### 策略二：关键频道监控

**目标**: 监控最重要的频道（公告、讨论、alpha）

**频道优先级**:
```python
CHANNEL_PRIORITY = {
    "announcements": 100,      # 官方公告 - 最高优先级
    "general": 80,             # 综合讨论
    "alpha": 90,               # alpha频道
    "airdrop": 85,             # 空投信息
    "whitelist": 85,           # 白名单
    "partnerships": 75,        # 合作公告
    "development": 70,         # 开发进展
    "trading": 60,             # 交易讨论
    "memes": 20,               # 表情包（低优先级）
}

def monitor_key_channels(guild_id: int):
    """监控关键频道"""
    
    guild = client.get_guild(guild_id)
    
    for channel in guild.text_channels:
        # 计算频道重要性
        importance = calculate_channel_importance(channel)
        
        if importance > 60:
            # 添加到监控列表
            add_to_monitor(channel)


def calculate_channel_importance(channel) -> int:
    """计算频道重要性"""
    
    score = 0
    
    # 基于频道名称
    for keyword, priority in CHANNEL_PRIORITY.items():
        if keyword in channel.name.lower():
            score = max(score, priority)
    
    # 基于消息频率
    recent_messages = get_recent_message_count(channel, hours=24)
    score += min(20, recent_messages / 50)
    
    # 基于成员互动
    # ...
    
    return int(score)
```

#### 策略三：实时消息捕获

**目标**: 捕获所有关键消息，特别是公告和alpha信息

```python
import discord
from discord.ext import commands

# 创建Bot
intents = discord.Intents.default()
intents.message_content = True
intents.guilds = True
intents.members = True

bot = commands.Bot(command_prefix='!', intents=intents)


@bot.event
async def on_ready():
    """Bot就绪"""
    logger.info(f'✅ Discord Bot logged in as {bot.user}')
    logger.info(f'监控 {len(bot.guilds)} 个服务器')


@bot.event
async def on_message(message):
    """处理新消息"""
    
    # 忽略bot自己的消息
    if message.author.bot:
        return
    
    # 只处理关键频道
    if not is_key_channel(message.channel):
        return
    
    # 提取关键信息
    key_info = extract_key_information({
        "text": message.content,
        "channel": message.channel.name,
        "guild": message.guild.name,
        "author": message.author.name,
        "timestamp": message.created_at
    })
    
    # 如果是重要消息
    if key_info and key_info["importance"] > 80:
        # 立即推送
        await send_alert(key_info)
    
    # 保存所有消息
    await save_to_database({
        "message_id": message.id,
        "channel_id": message.channel.id,
        "guild_id": message.guild.id,
        "author_id": message.author.id,
        "content": message.content,
        "created_at": message.created_at,
        "attachments": [att.url for att in message.attachments],
        "embeds": len(message.embeds),
    })


@bot.event
async def on_guild_join(guild):
    """加入新服务器"""
    logger.info(f'✅ 加入服务器: {guild.name}')
    
    # 分析服务器
    analysis = analyze_guild(guild)
    save_guild_info(analysis)
    
    # 识别关键频道
    key_channels = identify_key_channels(guild)
    for channel in key_channels:
        logger.info(f'  - 关键频道: #{channel.name}')
```

#### 策略四：社区活跃度分析

**目标**: 评估项目社区的真实活跃度

```python
async def analyze_community_activity(guild_id: int) -> dict:
    """分析社区活跃度"""
    
    guild = bot.get_guild(guild_id)
    
    # 1. 成员统计
    total_members = guild.member_count
    online_members = len([m for m in guild.members if m.status != discord.Status.offline])
    
    # 2. 消息统计（过去24小时）
    message_count = 0
    unique_authors = set()
    
    for channel in guild.text_channels:
        try:
            async for message in channel.history(limit=100, after=datetime.now() - timedelta(hours=24)):
                message_count += 1
                unique_authors.add(message.author.id)
        except:
            pass  # 无权限访问的频道
    
    # 3. 互动质量
    avg_message_length = calculate_avg_message_length(guild)
    bot_ratio = count_bot_messages(guild) / message_count if message_count > 0 else 0
    
    # 4. 计算活跃度分数
    activity_score = (
        min(30, total_members / 1000) +              # 成员数（30分）
        min(30, message_count / 100) +               # 消息量（30分）
        min(20, len(unique_authors) / 50) +          # 活跃用户（20分）
        min(20, (1 - bot_ratio) * 20)                # 真人比例（20分）
    )
    
    return {
        "guild_id": guild_id,
        "guild_name": guild.name,
        "total_members": total_members,
        "online_members": online_members,
        "daily_messages": message_count,
        "active_users": len(unique_authors),
        "bot_ratio": f"{bot_ratio*100:.1f}%",
        "activity_score": int(activity_score),
        "activity_level": "High" if activity_score > 70 else "Medium" if activity_score > 40 else "Low"
    }
```

### 3.3 数据结构设计

```python
class DiscordData(BaseModel):
    """Discord采集数据模型"""
    
    # 基础信息
    message_id: int
    channel_id: int
    channel_name: str
    guild_id: int
    guild_name: str
    
    # 作者信息
    author_id: int
    author_name: str
    author_discriminator: str
    author_is_bot: bool
    author_roles: List[str]
    
    # 消息内容
    content: str
    created_at: datetime
    edited_at: Optional[datetime]
    
    # 互动数据
    reactions: List[Dict[str, int]]  # [{emoji: count}]
    reply_count: int
    
    # 附件
    attachments: List[str]  # URLs
    embeds: List[dict]
    
    # 引用
    referenced_message_id: Optional[int]
    is_thread_starter: bool
    
    # 实体提取
    mentions: List[str]
    role_mentions: List[str]
    urls: List[str]
    
    # 分析结果
    key_information_type: Optional[str]
    importance_score: int
    
    # 采集元信息
    collected_at: datetime


class DiscordGuildInfo(BaseModel):
    """Discord服务器信息"""
    
    guild_id: int
    name: str
    description: Optional[str]
    
    # 规模
    member_count: int
    online_count: int
    
    # 验证
    is_verified: bool
    is_partnered: bool
    
    # 关联项目
    related_project: Optional[str]
    is_official: bool
    
    # 活跃度
    activity_score: int
    daily_message_count: int
    active_user_count: int
    
    # 频道统计
    text_channel_count: int
    key_channels: List[str]
    
    created_at: datetime
    joined_at: datetime
    last_checked: datetime
```

### 3.4 验收标准

**功能验收**:
- ✅ 能够加入 50+ Discord服务器
- ✅ 能够实时监控关键频道
- ✅ 能够识别公告和重要消息
- ✅ 能够分析社区活跃度

**性能验收**:
- ✅ 日采集量 > 2,000条消息
- ✅ 消息延迟 < 5秒
- ✅ 官方公告捕获率 > 95%

**质量验收**:
- ✅ 垃圾消息过滤率 > 80%
- ✅ 社区活跃度评估准确率 > 85%

---

## 📝 平台四: Medium

### 4.1 平台特点

**为什么重要**:
- ✅ 项目深度文章发布地
- ✅ 技术解析、路线图、更新日志
- ✅ 投资人分析文章
- ✅ 项目方官方博客

**数据特点**:
- 长文本（1000-5000字）
- 结构化内容
- SEO友好
- 订阅制

### 4.2 采集策略

#### 策略一：RSS订阅

**目标**: 订阅所有Web3相关的Medium出版物和作者

```python
import feedparser

MEDIUM_RSS_SOURCES = [
    # 顶级出版物
    "https://medium.com/feed/@VitalikButerin",
    "https://medium.com/feed/@a16z",
    "https://medium.com/feed/ethereum-optimism",
    "https://medium.com/feed/coinmonks",
    "https://medium.com/feed/coinbase",
    
    # 标签订阅
    "https://medium.com/feed/tag/web3",
    "https://medium.com/feed/tag/defi",
    "https://medium.com/feed/tag/cryptocurrency",
]

def collect_medium_articles():
    """采集Medium文章"""
    
    articles = []
    
    for rss_url in MEDIUM_RSS_SOURCES:
        feed = feedparser.parse(rss_url)
        
        for entry in feed.entries:
            article = {
                "title": entry.title,
                "url": entry.link,
                "author": entry.author,
                "published": entry.published_parsed,
                "summary": entry.summary,
                "tags": entry.get("tags", []),
            }
            
            # 只采集Web3相关
            if is_web3_related(article):
                articles.append(article)
    
    return articles
```

#### 策略二：关键词搜索

**目标**: 主动搜索特定关键词的文章

```python
def search_medium(keyword: str, max_results: int = 50):
    """搜索Medium文章"""
    
    # Medium没有官方API，使用Google搜索
    query = f"site:medium.com {keyword}"
    
    results = google_search(query, num_results=max_results)
    
    articles = []
    for result in results:
        article = scrape_medium_article(result["url"])
        articles.append(article)
    
    return articles
```

#### 策略三：全文提取与分析

**目标**: 提取文章全文并进行深度分析

```python
from bs4 import BeautifulSoup
import requests

def scrape_medium_article(url: str) -> dict:
    """爬取Medium文章"""
    
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # 提取标题
    title = soup.find('h1').text
    
    # 提取作者
    author = soup.find('a', {'data-action': 'show-user-card'}).text
    
    # 提取发布时间
    published = soup.find('time')['datetime']
    
    # 提取正文（Medium的文章内容在article标签中）
    article_body = soup.find('article')
    paragraphs = article_body.find_all('p')
    full_text = '\n\n'.join([p.text for p in paragraphs])
    
    # 提取统计数据
    claps = extract_claps(soup)
    responses = extract_responses(soup)
    
    # 提取提及的项目
    mentioned_projects = extract_project_mentions(full_text)
    
    return {
        "url": url,
        "title": title,
        "author": author,
        "published": published,
        "full_text": full_text,
        "word_count": len(full_text.split()),
        "claps": claps,
        "responses": responses,
        "mentioned_projects": mentioned_projects,
    }


def analyze_medium_article(article: dict) -> dict:
    """分析Medium文章"""
    
    # AI提取关键信息
    analysis = ai_analyze_article(article["full_text"])
    
    return {
        "article_url": article["url"],
        
        # 主题分类
        "topic": analysis["topic"],  # Technical, Investment, Update, Tutorial
        
        # 提及的项目
        "projects": analysis["projects"],
        
        # 关键洞察
        "key_insights": analysis["insights"],
        
        # 情感倾向
        "sentiment": analysis["sentiment"],  # Bullish, Neutral, Bearish
        
        # 重要性评分
        "importance": analysis["importance"],  # 0-100
        
        # 是否包含Alpha信息
        "has_alpha": analysis["has_alpha"],
    }
```

### 4.3 数据结构设计

```python
class MediumArticle(BaseModel):
    """Medium文章数据模型"""
    
    # 基础信息
    url: str
    title: str
    subtitle: Optional[str]
    
    # 作者信息
    author: str
    author_url: str
    author_followers: Optional[int]
    
    # 发布信息
    published_at: datetime
    updated_at: Optional[datetime]
    publication: Optional[str]  # 所属出版物
    
    # 内容
    full_text: str
    word_count: int
    reading_time_minutes: int
    
    # 互动数据
    claps: int
    responses: int
    
    # 分类
    tags: List[str]
    topics: List[str]
    
    # 提及的项目
    mentioned_projects: List[str]
    
    # AI分析结果
    article_type: str  # Technical, Investment, Update, Tutorial
    key_insights: List[str]
    sentiment: str
    importance_score: int
    has_alpha: bool
    
    # 采集元信息
    collected_at: datetime
    source: str  # rss, search, manual
```

### 4.4 验收标准

**功能验收**:
- ✅ 能够订阅 50+ RSS源
- ✅ 能够搜索特定关键词文章
- ✅ 能够提取文章全文
- ✅ 能够识别提及的项目

**性能验收**:
- ✅ 日采集量 > 200篇文章
- ✅ 全文提取准确率 > 95%

**质量验收**:
- ✅ 项目提及识别准确率 > 85%
- ✅ 文章分类准确率 > 80%

---

## 📊 数据采集总控系统

### 统一调度器

```python
from celery import Celery
from celery.schedules import crontab

app = Celery('multi_platform_collector')

# Twitter采集任务
@app.task
def collect_twitter():
    collector = TwitterCollector()
    data = collector.collect_and_extract(hours=1)
    process_and_save(data, platform="twitter")

# Telegram采集任务
@app.task
def collect_telegram():
    collector = TelegramCollector()
    data = await collector.collect_and_extract(hours=1)
    process_and_save(data, platform="telegram")

# Discord采集任务
@app.task
def collect_discord():
    # Discord通过Bot实时监听，无需定时任务
    pass

# Medium采集任务
@app.task
def collect_medium():
    collector = MediumCollector()
    data = collector.collect_articles()
    process_and_save(data, platform="medium")


# 任务调度配置
app.conf.beat_schedule = {
    'twitter-every-5min': {
        'task': 'collect_twitter',
        'schedule': 300.0,
    },
    'telegram-every-15min': {
        'task': 'collect_telegram',
        'schedule': 900.0,
    },
    'medium-every-hour': {
        'task': 'collect_medium',
        'schedule': 3600.0,
    },
}
```

### 数据归一化

```python
class UnifiedProjectData(BaseModel):
    """统一的项目数据模型"""
    
    # 项目标识
    project_name: str
    project_id: Optional[str]
    
    # 发现信息
    first_discovered_at: datetime
    discovery_source: str  # twitter, telegram, discord, medium
    discovery_context: str
    
    # 多平台数据聚合
    twitter_mentions: int
    telegram_mentions: int
    discord_mentions: int
    medium_mentions: int
    
    # 综合指标
    total_mentions: int
    mention_growth_rate: float
    cross_platform_consistency: float  # 0-1，跨平台一致性
    
    # 信号强度
    overall_signal_strength: int  # 0-100
    
    # 最后更新
    last_updated: datetime


def aggregate_multi_platform_data(project_name: str) -> UnifiedProjectData:
    """聚合多平台数据"""
    
    # 从各平台获取数据
    twitter_data = get_twitter_mentions(project_name)
    telegram_data = get_telegram_mentions(project_name)
    discord_data = get_discord_mentions(project_name)
    medium_data = get_medium_mentions(project_name)
    
    # 计算综合指标
    total = len(twitter_data) + len(telegram_data) + len(discord_data) + len(medium_data)
    
    # 计算增长率
    growth = calculate_mention_growth(project_name, days=7)
    
    # 计算跨平台一致性
    consistency = calculate_cross_platform_consistency(
        twitter_data, telegram_data, discord_data, medium_data
    )
    
    # 计算综合信号强度
    signal = calculate_overall_signal(
        twitter_data, telegram_data, discord_data, medium_data
    )
    
    return UnifiedProjectData(
        project_name=project_name,
        first_discovered_at=find_earliest_mention(project_name),
        discovery_source=find_first_source(project_name),
        twitter_mentions=len(twitter_data),
        telegram_mentions=len(telegram_data),
        discord_mentions=len(discord_data),
        medium_mentions=len(medium_data),
        total_mentions=total,
        mention_growth_rate=growth,
        cross_platform_consistency=consistency,
        overall_signal_strength=signal,
        last_updated=datetime.now()
    )
```

---

## 📋 实施计划

### Week 1-2: 第一梯队平台
- **Day 1-3**: Twitter增强（评论区挖掘、话题追踪）
- **Day 4-6**: Telegram增强（群组监控、官方验证）
- **Day 7-10**: Discord实现（Bot开发、频道监控）
- **Day 11-14**: Medium实现（RSS订阅、全文提取）

### Week 3-4: 数据处理与分析
- **Day 15-18**: 数据归一化与去重
- **Day 19-22**: 跨平台交叉验证
- **Day 23-26**: 热度追踪算法
- **Day 27-28**: 测试与优化

---

**文档版本**: v1.0  
**最后更新**: 2025-10-04  
**负责人**: 产品经理

